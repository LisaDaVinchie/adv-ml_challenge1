{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Challenge 1*: A **kernel** methods / **DL** pipeline for the FashionMNIST dataset\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_Challenge_1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits import mplot3d\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, accuracy_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Used to save data into files\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# Used to measure time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import train and test dataset, scale them and convert them to data loaders\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select some images from the training and test dataset\n",
    "\n",
    "subset_size = 10000\n",
    "\n",
    "## set a seed for randperm\n",
    "th.manual_seed(42)\n",
    "\n",
    "idx = th.randperm(len(train_dataset))[:subset_size]\n",
    "\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "\n",
    "train_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "\n",
    "idx = th.randperm(len(test_dataset))[:subset_size]\n",
    "\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "\n",
    "test_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "\n",
    "del(idx)\n",
    "del(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the images and their labels to numpy arrays and reshape them to vectors\n",
    "\n",
    "labels_train = []\n",
    "train_subset = []\n",
    "for batch in train_subset_loader:\n",
    "    data, labels = batch\n",
    "    \n",
    "    train_subset.append(data.numpy().reshape(1, -1))\n",
    "    labels_train.append(labels.numpy())\n",
    "\n",
    "train_subset_scaled = np.array(train_subset).reshape(subset_size, -1)\n",
    "labels_train = np.array(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subset = []\n",
    "# labels_test = []\n",
    "\n",
    "# for batch in test_subset_loader:\n",
    "#     data, labels = batch\n",
    "    \n",
    "#     test_subset.append(data.numpy().reshape(1, -1))\n",
    "#     labels_test.append(labels.numpy())\n",
    "\n",
    "# test_subset_scaled = np.array(test_subset).reshape(subset_size, -1)\n",
    "# labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of labels for better understanding\n",
    "description = {0: \"T-shirt/top\", \n",
    "               1: \"Trouser\", \n",
    "               2: \"Pullover\", \n",
    "               3: \"Dress\", \n",
    "               4: \"Coat\", \n",
    "               5: \"Sandal\", \n",
    "               6: \"Shirt\", \n",
    "               7: \"Sneaker\", \n",
    "               8: \"Bag\", \n",
    "               9: \"Ankle boot\"}\n",
    "\n",
    "ticks = list(description.keys())\n",
    "tick_labels = list(description.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions to save and load data from pickle files\n",
    "\n",
    "# def save_data(data, filename):\n",
    "#     if not os.path.exists(filename):\n",
    "#         with open(filename, \"wb\") as f:\n",
    "#             pkl.dump(data, f)\n",
    "\n",
    "# def load_data(filename):\n",
    "#     if os.path.exists(filename):\n",
    "#         with open(filename, \"rb\") as f:\n",
    "#             data = pkl.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the color map for the plots\n",
    "colors_rgb = [\n",
    "    (33, 240, 182),\n",
    "    (21, 122, 72),\n",
    "    (155, 209, 198),\n",
    "    (16, 85, 138),\n",
    "    (172, 139, 248),\n",
    "    (133, 22, 87),\n",
    "    (197, 81, 220),\n",
    "    (56, 181, 252),\n",
    "    (18, 85, 211),\n",
    "    (171, 230, 91),\n",
    "]\n",
    "colors_rgb_normalized = colors_rgb_normalized = np.array(colors_rgb) / 255.0\n",
    "cmap = ListedColormap(colors_rgb_normalized)\n",
    "plt.rcParams[\"ps.useafm\"] = True\n",
    "title_dict = {\n",
    "    \"fontname\": \"Sans-serif\",\n",
    "    \"fontsize\": 16,\n",
    "    \"fontweight\": \"bold\",\n",
    "}\n",
    "\n",
    "bar_color = (16, 85, 138)\n",
    "bar_rgb_color = np.array(bar_color) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform linear PCA\n",
    "\n",
    "model = PCA(n_components = 3)\n",
    "data_pca_linear = model.fit_transform(train_subset_scaled)\n",
    "\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first two principal components\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_linear[:, 0], data_pca_linear[:, 1], c=labels_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plt.savefig(\"Report/pca_linear_2comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first three principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_linear[:, 0],\n",
    "        data_pca_linear[:, 1],\n",
    "        data_pca_linear[:, 2],\n",
    "        c=labels_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "\n",
    "# legend = ax.legend(*p.legend_elements(), loc=\"right\", title=\"Classes\")\n",
    "# ax.add_artist(legend)\n",
    "ax.dist = 13\n",
    "#plt.show()\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p\n",
    "\n",
    "plt.savefig(\"Report/pca_linear_3comps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "The data does not seem to be well separated, so finding the right hyperplane for classification will be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Perform kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel\n",
    "    \n",
    "kernel_pca = KernelPCA(kernel=\"rbf\", n_components = 3)\n",
    "data_pca_rbf = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_rbf[:, 0], data_pca_rbf[:, 1], c=labels_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"Report/pca_rbf_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del (p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_rbf[:, 0],\n",
    "        data_pca_rbf[:, 1],\n",
    "        data_pca_rbf[:, 2],\n",
    "        c=labels_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "ax.dist = 13\n",
    "###plt.show()\n",
    "\n",
    "plt.savefig(\"Report/pca_rbf_3comps.png\")\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = 5 / 784)\n",
    "data_pca_rbf = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "plt.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], c = labels_train, marker='.', cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel, tune gamma to separate clusters\n",
    "    \n",
    "gamma = np.array([(1/10)*(1/784), 1/784, 10 * (1/784)])\n",
    "\n",
    "data_pca_rbf = np.ndarray((10000, 3, len(gamma)))\n",
    "\n",
    "for i in range(len(gamma)):\n",
    "    kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = gamma[i])\n",
    "    data_pca_rbf[:, :, i] = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for different gammas\n",
    "\n",
    "gammas = [\"$\\\\frac{1}{5} * \\\\frac{1}{784}$\", \"$\\\\frac{1}{784}$\", \"$5 * \\\\frac{1}{784}$\"]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(17, 6), dpi=200)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    p = ax.scatter(\n",
    "        data_pca_rbf[:, 0, i], data_pca_rbf[:, 1, i], c=labels_train, marker=\".\", cmap=cmap\n",
    "    )\n",
    "    ax.set_title(\"Gamma = \" + gammas[i])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.savefig(\"Report/pca_rbf_different_gammas.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the range of the parameter gamma\n",
    "gammas = np.arange(\n",
    "    1 / 784 - 5 * (1 / 784) * (1 / 10),\n",
    "    1 / 784 + 5 * (1 / 784) * (1 / 10),\n",
    "    (1 / 784) * (1 / 10),\n",
    ")\n",
    "\n",
    "## Extract eigenvalues\n",
    "n_components = 3\n",
    "eigenvalues_rbf = np.empty((len(gammas), n_components))\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    kernel_pca = KernelPCA(kernel=\"rbf\", n_components=n_components, gamma=gammas[i])\n",
    "    eigenvalues_rbf[i] = kernel_pca.fit(train_subset_scaled).eigenvalues_\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(30, 10))\n",
    "# Create 10 random plots\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    x = np.arange(1, len(eigenvalues_rbf[i, :]) + 1, 1)\n",
    "    # Plot the data on the corresponding axis\n",
    "    ax.bar(x, eigenvalues_rbf[i, :], color=bar_rgb_color)\n",
    "    ax.set_ylim(0, 500)\n",
    "    ax.set_xticks(range(1, 4))\n",
    "    # ax.set_xlabel('Component')\n",
    "    # ax.set_ylabel('Eigenvalue')\n",
    "    ax.set_title(\"Gamma = \" + str(np.round(gammas[i], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Perform kPCA using another kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel poly\n",
    "\n",
    "kernel_pca = KernelPCA(kernel = \"poly\", n_components = 3)\n",
    "\n",
    "data_pca_poly = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_poly[:, 0], data_pca_poly[:, 1], c=labels_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "\n",
    "plt.savefig(\"Report/pca_poly_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del (p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_poly[:, 0],\n",
    "        data_pca_poly[:, 1],\n",
    "        data_pca_poly[:, 2],\n",
    "        c=labels_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "ax.dist = 13\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/pca_poly_3comps.png\")\n",
    "\n",
    "del (p, fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel sigmoid\n",
    "kernel_pca = KernelPCA(kernel=\"sigmoid\", n_components = 10)\n",
    "\n",
    "data_pca_sigmoid = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "eigenvalues_sigmoid = kernel_pca.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c=labels_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.savefig(\"Report/pca_sigmoid_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del (p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_sigmoid[:, 0],\n",
    "        data_pca_sigmoid[:, 1],\n",
    "        data_pca_sigmoid[:, 2],\n",
    "        c=labels_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "\n",
    "ax.dist = 13\n",
    "##plt.show()\n",
    "plt.savefig(\"Report/pca_sigmoid_3comps.png\")\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_linear, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_rbf, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_poly, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels_train.reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: linear: {DB_score[0]:.4f} | rbf: {DB_score[1]:.4f} | poly: {DB_score[2]:.4f} | sigmoid: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with different techniques\n",
    "\n",
    "labels_Kmeans = KMeans(n_clusters = 10, n_init=10).fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Spectral = SpectralClustering(n_clusters = 10, affinity='nearest_neighbors').fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Gaussian = GaussianMixture(n_components = 10).fit(data_pca_sigmoid).predict(data_pca_sigmoid)\n",
    "\n",
    "labels = np.array([labels_train.reshape(subset_size), labels_Kmeans, labels_Spectral, labels_Gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results and compare them with the original clustering\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True, dpi=200)\n",
    "\n",
    "title_names = [\"Original\", \"K Means\", \"Spectral Clustering\", \"Gaussian Mixture\"]\n",
    "\n",
    "for ax, i in zip(axs.flat, range(4)):\n",
    "    ax.scatter(\n",
    "        data_pca_sigmoid[:, 0],\n",
    "        data_pca_sigmoid[:, 1],\n",
    "        c=labels[i, :],\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "    ax.set_title(title_names[i], fontweight=\"bold\", fontsize=13)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.savefig(\"Report/unsupervised_clustering.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Adjusted Rand Index\n",
    "\n",
    "ARI = np.empty(3)\n",
    "\n",
    "for i in range(3):\n",
    "    ARI[i] = adjusted_rand_score(labels[0, :], labels[i + 1, :])\n",
    "    print(f\"Adjusted Rand Index for {title_names[i + 1]}: {ARI[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[0, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[1, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[2, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[3, :].reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: original: {DB_score[0]:.4f} | KMeans: {DB_score[1]:.4f} | Spectral: {DB_score[2]:.4f} | Gaussian: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "As we can see, label assignment performed poorly. This, probably, because the clusters are very close to each other and not clearly separated.\n",
    "\n",
    "#### b\n",
    "As we can see from the plot below, there is a clear elbow on the third component. This suggests that 10 does not reflect the actual knee point of the spectrum of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalues obtained with the sigmoid method\n",
    "\n",
    "plt.plot(np.arange(1, len(eigenvalues_sigmoid) + 1, 1), eigenvalues_sigmoid)\n",
    "\n",
    "plt.xticks(np.arange(1, 11, 1))\n",
    "\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "# plt.savefig(\"Report/eigenvalues_sigmoid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized plot of the eigenvalues\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "plt.bar(\n",
    "    x=np.arange(1, len(eigenvalues_sigmoid) + 1),\n",
    "    height=eigenvalues_sigmoid,\n",
    "    color=bar_rgb_color,\n",
    ")\n",
    "xticks = np.arange(1, len(eigenvalues_sigmoid) + 1, 1)\n",
    "plt.xticks(xticks)\n",
    "plt.xlabel(\"Component\", fontsize=11)\n",
    "plt.ylabel(\"Eigenvalue\", fontsize=11, rotation=0, labelpad=35)\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/eigenvalues_sigmoid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized plot of the eigenvalues\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "plt.bar(\n",
    "    x=np.arange(1, len(eigenvalues_sigmoid) + 1),\n",
    "    height=eigenvalues_sigmoid / np.max(eigenvalues_sigmoid),\n",
    "    color=bar_rgb_color,\n",
    ")\n",
    "xticks = np.arange(1, len(eigenvalues_sigmoid) + 1, 1)\n",
    "plt.xticks(xticks)\n",
    "plt.xlabel(\"Component\", fontsize=11)\n",
    "plt.ylabel(\"Eigenvalue\", fontsize=11, rotation=0, labelpad=35)\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/eigenvalues_sigmoid_normalized.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ARI)\n",
    "del(eigenvalues_sigmoid)\n",
    "del(labels_Kmeans)\n",
    "del(labels_Gaussian)\n",
    "del(ax)\n",
    "del(axs)\n",
    "del(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels_Spectral, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: kernel SVM with different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel\n",
    "\n",
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_linear = classifier.predict(x_test)\n",
    "\n",
    "acc_linear = accuracy_score(y_test, label_predict_SVC_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel\n",
    "\n",
    "classifier = SVC(kernel = \"rbf\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test)\n",
    "\n",
    "acc_rbf = accuracy_score(y_test, label_predict_SVC_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel\n",
    "\n",
    "classifier = SVC(kernel = \"poly\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_poly = classifier.predict(x_test)\n",
    "\n",
    "acc_poly = accuracy_score(y_test, label_predict_SVC_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid kernel\n",
    "\n",
    "classifier = SVC(kernel = \"sigmoid\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_sigmoid = classifier.predict(x_test)\n",
    "\n",
    "acc_sigmoid = accuracy_score(y_test, label_predict_SVC_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: linear: {acc_linear:.2f} | rbf: {acc_rbf:.2f} | poly: {acc_poly:2f} | sigmoid: {acc_sigmoid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_SVC_unsupervised = np.concatenate((y_test.reshape(len(y_test), 1),\n",
    "                                            label_predict_SVC_linear.reshape(len(y_test), 1),\n",
    "                                            label_predict_SVC_rbf.reshape(len(y_test), 1),\n",
    "                                            label_predict_SVC_poly.reshape(len(y_test), 1),\n",
    "                                            label_predict_SVC_sigmoid.reshape(len(y_test), 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "titles = [\"Original\", \"Linear\", \"RBF\", \"Polynomial\", \"Sigmoid\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "gs = gridspec.GridSpec(2, 6)  # , width_ratios=[1, 1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Create subplots\n",
    "ax1 = plt.subplot(gs[0, :2])\n",
    "ax2 = plt.subplot(gs[0, 2:4])\n",
    "ax3 = plt.subplot(gs[0, 4:])\n",
    "ax4 = plt.subplot(gs[1, 1:3])\n",
    "ax5 = plt.subplot(gs[1, 3:5])\n",
    "\n",
    "# Plot data in subplots (replace with your actual plotting code)\n",
    "ax1.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 0], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax1.title.set_text(titles[0])\n",
    "\n",
    "ax2.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 1], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax2.title.set_text(titles[1])\n",
    "\n",
    "ax3.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 2], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax3.title.set_text(titles[2])\n",
    "\n",
    "ax4.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 3], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax4.title.set_text(titles[3])\n",
    "\n",
    "ax5.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 4], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax5.title.set_text(titles[4])\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/unsupervised_SVC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass data to tensors\n",
    "\n",
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train, dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test, dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide if you want to train muoltiple models with different hyperparameters\n",
    "train_multiple_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions needed to calculate the accuracy\n",
    "\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function used to train the model\n",
    "\n",
    "def train_model(epochs, train_loader, criterion, optimizer, device, model):\n",
    "    _batch_losses = []\n",
    "    \n",
    "    _model = model\n",
    "    for _ in trange(epochs):\n",
    "        _model = _model.train()\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for _, (_images, _labels) in enumerate(train_loader):\n",
    "            _images = _images.to(device)\n",
    "            _labels = _labels.to(device)\n",
    "\n",
    "            _logits = _model(_images)\n",
    "            _loss = criterion(_logits, _labels)\n",
    "            _batch_losses.append(_loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function used to get labels\n",
    "def get_predicted_labels(model, test_data, device):\n",
    "    test_data_tensor = th.tensor(test_data.reshape(-1, 1, 28, 28))\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    labels = []\n",
    "    with th.no_grad():\n",
    "        for i in range(test_data_tensor.shape[0]):\n",
    "            data = test_data_tensor[i].reshape(1, 1, 28, 28)\n",
    "            pred = model(data.to(device))\n",
    "            labels.append(th.argmax(pred).item())\n",
    "            \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Fully Connected Neural Network\n",
    "\n",
    "class FullyConnectedNN_1layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes):\n",
    "        \n",
    "        super(FullyConnectedNN_1layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_FCNN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and calculate accuracy on the test set\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose for which epochs to train the model\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "\n",
    "    # Store the accuracies and predicted labels in two arrays\n",
    "    trained_acc_FC_1l = []\n",
    "    labels_FC_1l = np.ndarray((3000, len(epochs)))\n",
    "    \n",
    "    # Choose the loss  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create a vectors to store the training time (column 1) for each epoch (column 0)\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in epochs:\n",
    "        model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_FCNN)\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        # Keep track of how much time is required to train the model\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = i,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        times[i - 1, 0] = i\n",
    "        times[i - 1, 1] = end_time - start_time\n",
    "        \n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_1l.append(trained_acc)\n",
    "        \n",
    "        \n",
    "        labels_FC_1l[:, i - 1] = get_predicted_labels(model = model, test_data = x_test, device = device)\n",
    "\n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i - 1, 1]:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy as a function of the number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(np.arange(1, len(trained_acc_FC_1l) + 1, 1),\n",
    "            trained_acc_FC_1l,\n",
    "            color=bar_rgb_color\n",
    "    )\n",
    "    plt.xticks(np.arange(1, len(trained_acc_FC_1l) + 1, 2))\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_FCNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "\n",
    "class FullyConnectedNN_2layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes, hidden_features):\n",
    "        \n",
    "        super(FullyConnectedNN_2layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = hidden_features)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = hidden_features,\n",
    "                             out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaning_rate_FCNN = 0.01\n",
    "epochs_FCNN = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how accuracy vary with the number of hidden neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose for which numbers of neurons to train the model\n",
    "    neurons = np.arange(50, 10050, 1000)\n",
    "\n",
    "    trained_acc_FC_2l_neurons = []\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "\n",
    "    times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i])\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = leaning_rate_FCNN)\n",
    "        \n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "            \n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs_FCNN,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i, 0] = neurons[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        \n",
    "                \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_2l_neurons.append(trained_acc)\n",
    "        \n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        model = model.eval()\n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_FCNN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy wrt number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons,\n",
    "            trained_acc_FC_2l_neurons,\n",
    "            color=bar_rgb_color\n",
    "            )\n",
    "    plt.ylim((50, 100))\n",
    "    plt.xlabel(\"Number of hidden neurons\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.savefig(\"Report/ex3_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how accuracy varies depending on the number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Define vector to keep all the accuracies, that we will plot\n",
    "    trained_acc_FC_2l = []\n",
    "\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "    # Define an array to keep all the predicted labels\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    # Choose loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 850)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i, 0] = epochs[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "        \n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        model = model.eval()\n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy of both networks\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        np.arange(1, 21, 1),\n",
    "        trained_acc_FC_2l_neurons,\n",
    "        color=bar_rgb_color,\n",
    "        label=\"2 layers\",\n",
    "    )\n",
    "    plt.plot(neurons, trained_acc_FC_1l, color=(16, 85, 138), label=\"1 layer\")\n",
    "    plt.plot\n",
    "    plt.ylim((50, 100))\n",
    "    plt.xlabel(\"Number of hidden neurons\")\n",
    "    plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_FCNN = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        np.arange(1, 21, 1),\n",
    "        trained_acc_FC_1l,\n",
    "        color=bar_rgb_color,\n",
    "        label = \"1 layer\")\n",
    "    plt.plot(\n",
    "        np.arange(1, 21, 1),\n",
    "        trained_acc_FC_2l, \n",
    "        color=np.array((133, 22, 87)) / 255.0,\n",
    "        label = \"2 layers\"\n",
    "    )\n",
    "    plt.xticks(np.arange(1, 21, 2))\n",
    "    plt.ylim((50, 100))\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"Report/ex3-FCNN-comparison-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "\n",
    "times = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(20):\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = hidden_size_FCNN)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_FCNN)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = epochs_FCNN,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "\n",
    "    print(f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\")\n",
    "    times.append(elapsed_time)\n",
    "    test_accuracies.append(trained_acc)\n",
    "\n",
    "model = model.eval()\n",
    "labels_FCNN = get_predicted_labels(test_data = x_test, device = device, model = model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average time: {np.mean(times):.2f} s | Max time: {np.max(times):.2f} s | Min time: {np.min(times):.2f} s\")\n",
    "print(f\"Average accuracy: {np.mean(test_accuracies):.2f}% | Max accuracy: {np.max(test_accuracies):.2f}% | Min accuracy: {np.min(test_accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size):\n",
    "        super(CNN_1layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = self._dimensions[1],\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_CNN = 2\n",
    "\n",
    "learning_rate_CNN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    model = CNN_1layer(n_classes = 10, kernel_size = kernel_size_CNN, input_size=[7000, 1, 28, 28])\n",
    "\n",
    "    # Choose loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define vector to keep all the accuracies, that we will plot\n",
    "    trained_acc_CNN_1l = []\n",
    "\n",
    "    epochs = [1, 10, 20]\n",
    "\n",
    "    CNN_labels = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i - 1, 0] = epochs[i]\n",
    "        times[i - 1, 1] = time.time() - start_time\n",
    "            \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_1l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_CNN_1l, color=bar_rgb_color)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Accuracy', rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "\n",
    "class CNN_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size1, kernel_size2, input_size, hidden_size, pool_size1, pool_size2):\n",
    "        super(CNN_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels = self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size = kernel_size1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - kernel_size1 + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - kernel_size1 + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(pool_size1, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - pool_size1) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - pool_size1) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = hidden_size,\n",
    "                               out_channels = self._dimensions[1],\n",
    "                               kernel_size = kernel_size2)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - kernel_size2 + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - kernel_size2 + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(pool_size2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - pool_size2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - pool_size2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool_size_CNN = 2\n",
    "hidden_size_CNN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of epochs`\n",
    "\n",
    "if train_multiple_models:\n",
    "    epochs = np.arange(1, 11, 1)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trained_acc_CNN_2l = []\n",
    "\n",
    "    CNN_labels = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN, input_size = [7000, 1, 28, 28], hidden_size = hidden_size_CNN, pool_size1 = pool_size_CNN, pool_size2 = pool_size_CNN)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "\n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        times[i, 0] = epochs[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        epochs,\n",
    "        trained_acc_CNN_2l, \n",
    "        color=bar_rgb_color\n",
    "    )\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Accuracy', rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        [1, 5, 10], \n",
    "        trained_acc_CNN_1l, \n",
    "        color=bar_rgb_color, \n",
    "        label=\"1 layer\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        epochs,\n",
    "        trained_acc_CNN_2l,\n",
    "        color=np.array((133, 22, 87)) / 255.0,\n",
    "        label=\"2 layers\",\n",
    "    )\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.legend()\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3-CNN acc-comparison-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_CNN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trained_acc_CNN_2l = []\n",
    "\n",
    "    neurons = np.arange(50, 550, 100)\n",
    "\n",
    "    times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "    CNN_labels = np.ndarray((len(x_test), len(neurons)))\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN,  input_size=[7000, 1, 28, 28], hidden_size = neurons[i], pool_size1 = pool_size_CNN, pool_size2 = pool_size_CNN)\n",
    "        \n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "        model = model.train()\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs_CNN,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        times[i, 0] = neurons[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons per hidden layer\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        neurons, \n",
    "        trained_acc_CNN_2l,\n",
    "        color=bar_rgb_color\n",
    "    )\n",
    "    plt.xticks(neurons)\n",
    "    plt.xlabel('Number of neurons per hidden layer')\n",
    "    plt.ylabel('Accuracy', rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_list = {\n",
    "#     'pool_size1' : np.arange(1, 10, 1),\n",
    "#     'pool_size2' : np.arange(1, 10, 1),\n",
    "#     'kernel_size1' : np.arange(1, 10, 1),\n",
    "#     'kernel_size2' : np.arange(1, 10, 1),\n",
    "#     'lr' : [0.1, 0.01, 0.001, 0.0001]\n",
    "# }\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# n_samples = 10\n",
    "\n",
    "# chosen_parameters = np.ndarray((n_samples, 6), dtype=float)\n",
    "\n",
    "# for i in range(n_samples):\n",
    "#     pool_size1 = np.random.choice(param_list['pool_size1'])\n",
    "#     pool_size2 = np.random.choice(param_list['pool_size2'])\n",
    "#     kernel_size1 = np.random.choice(param_list['kernel_size1'])\n",
    "#     kernel_size2 = np.random.choice(param_list['kernel_size2'])\n",
    "    \n",
    "#     if kernel_size1 + kernel_size2 + pool_size1 + pool_size2 > 15:\n",
    "#         while kernel_size1 + kernel_size2 + pool_size1 + pool_size2 > 15:\n",
    "#             pool_size1 = np.random.choice(param_list['pool_size1'])\n",
    "#             pool_size2 = np.random.choice(param_list['pool_size2'])\n",
    "#             kernel_size1 = np.random.choice(param_list['kernel_size1'])\n",
    "#             kernel_size2 = np.random.choice(param_list['kernel_size2'])\n",
    "    \n",
    "#     lr = np.random.choice(param_list['lr'])\n",
    "    \n",
    "#     model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size1, kernel_size2 = kernel_size2, input_size=[7000, 1, 28, 28], hidden_size = kernel_size_CNN, pool_size1 = pool_size1, pool_size2 = pool_size2)\n",
    "\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     optimizer = th.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "#     model = model.eval()\n",
    "\n",
    "#     untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "#     model = model.train()\n",
    "\n",
    "#     model = train_model(epochs = epochs_CNN,\n",
    "#                         train_loader = data_train_loader,\n",
    "#                         criterion = criterion,\n",
    "#                         optimizer = optimizer,\n",
    "#                         device = device,\n",
    "#                         model = model)\n",
    "\n",
    "#     trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "#     chosen_parameters[i, 0] = pool_size1\n",
    "#     chosen_parameters[i, 1] = pool_size2\n",
    "#     chosen_parameters[i, 2] = kernel_size1\n",
    "#     chosen_parameters[i, 3] = kernel_size2\n",
    "#     chosen_parameters[i, 4] = lr\n",
    "#     chosen_parameters[i, 5] = trained_acc\n",
    "\n",
    "#     print(f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chosen_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 5, figsize=(30, 5), sharey = True)\n",
    "# titles = [\"Pooling size 1\", \"Pooling size 2\", \"Kernel size1\", \"Kernel size 2\", \"Learning rate\"]\n",
    "\n",
    "# for i, ax in enumerate(axs.flat):\n",
    "#     ax.plot(chosen_parameters[:, i], chosen_parameters[:, chosen_parameters.shape[1] - 1], marker = 'o', linestyle = '')\n",
    "#     ax.set_title(titles[i])\n",
    "#     ax.set_ylabel(titles[i])\n",
    "#     ax.set_xticks(np.unique(chosen_parameters[:, i]))\n",
    "#     ax.grid()\n",
    "\n",
    "# ax.set_xlabel(\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# lrs = np.unique(chosen_parameters[:, 2])\n",
    "\n",
    "# p0 = ax.scatter(chosen_parameters[:, 0][chosen_parameters[:, 2] == lrs[0]],\n",
    "#                 chosen_parameters[:, 1][chosen_parameters[:, 2] == lrs[0]],\n",
    "#                 chosen_parameters[:, 3][chosen_parameters[:, 2] == lrs[0]], marker='o', label = lrs[0])\n",
    "\n",
    "# p1 = ax.scatter(chosen_parameters[:, 0][chosen_parameters[:, 2] == lrs[1]],\n",
    "#                 chosen_parameters[:, 1][chosen_parameters[:, 2] == lrs[1]],\n",
    "#                 chosen_parameters[:, 3][chosen_parameters[:, 2] == lrs[1]], marker='o', label = lrs[1])\n",
    "\n",
    "# p2 = ax.scatter(chosen_parameters[:, 0][chosen_parameters[:, 2] == lrs[2]],\n",
    "#                 chosen_parameters[:, 1][chosen_parameters[:, 2] == lrs[2]],\n",
    "#                 chosen_parameters[:, 3][chosen_parameters[:, 2] == lrs[2]], marker='o', label = lrs[2])\n",
    "\n",
    "# p3 = ax.scatter(chosen_parameters[:, 0][chosen_parameters[:, 2] == lrs[3]],\n",
    "#                 chosen_parameters[:, 1][chosen_parameters[:, 2] == lrs[3]],\n",
    "#                 chosen_parameters[:, 3][chosen_parameters[:, 2] == lrs[3]], marker='o', label = lrs[3])\n",
    "\n",
    "# ax.set_xlabel('Pool size')\n",
    "# ax.set_ylabel('Kernel size')\n",
    "\n",
    "# ax.legend(loc=\"upper right\", title=\"Learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_list = {\n",
    "#     'pool_size1' : np.arange(2, 14, 1),\n",
    "#     'pool_size2' : np.arange(1, 13, 1),\n",
    "#     'kernel_size1' : np.arange(1, 14, 1),\n",
    "#     'kernel_size2' : np.arange(1, 14, 1),\n",
    "# }\n",
    "\n",
    "# epochs = 5\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# n_samples = 20\n",
    "\n",
    "# chosen_parameters = np.ndarray((n_samples, len(param_list) + 1), dtype=float)\n",
    "\n",
    "# acc_idx = chosen_parameters.shape[1] - 1\n",
    "\n",
    "# for i in range(n_samples):\n",
    "#     pool_size1 = np.random.choice(param_list['pool_size1'])\n",
    "#     pool_size2 = np.random.choice(param_list['pool_size2'])\n",
    "#     kernel_size1 = np.random.choice(param_list['kernel_size1'])\n",
    "#     kernel_size2 = np.random.choice(param_list['kernel_size2'])\n",
    "    \n",
    "#     if kernel_size1 + kernel_size2 + pool_size1 + pool_size2 > 15 or pool_size1 < pool_size2:\n",
    "#         while kernel_size1 + kernel_size2 + pool_size1 + pool_size2 > 15 or pool_size1 < pool_size2:\n",
    "#             pool_size1 = np.random.choice(param_list['pool_size1'])\n",
    "#             pool_size2 = np.random.choice(param_list['pool_size2'])\n",
    "#             kernel_size1 = np.random.choice(param_list['kernel_size1'])\n",
    "#             kernel_size2 = np.random.choice(param_list['kernel_size2'])\n",
    "    \n",
    "    \n",
    "#     model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size1, kernel_size2 = kernel_size2, input_size=[7000, 1, 28, 28], hidden_size = kernel_size_CNN, pool_size1 = pool_size1, pool_size2 = pool_size2)\n",
    "\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "\n",
    "#     model = model.eval()\n",
    "\n",
    "#     untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "#     model = model.train()\n",
    "\n",
    "#     model = train_model(epochs = epochs,\n",
    "#                         train_loader = data_train_loader,\n",
    "#                         criterion = criterion,\n",
    "#                         optimizer = optimizer,\n",
    "#                         device = device,\n",
    "#                         model = model)\n",
    "\n",
    "#     trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "#     chosen_parameters[i, 0] = pool_size1\n",
    "#     chosen_parameters[i, 1] = pool_size2\n",
    "#     chosen_parameters[i, 2] = kernel_size1\n",
    "#     chosen_parameters[i, 3] = kernel_size2\n",
    "#     chosen_parameters[i, acc_idx] = trained_acc\n",
    "\n",
    "#     print(f\"Iteration: {i} | Loss: {trained_loss:.4f} | Accuracy: {trained_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 4, figsize=(30, 5), sharey = True)\n",
    "# titles = [\"Pooling size 1\", \"Pooling size 2\", \"Kernel size1\", \"Kernel size 2\"]\n",
    "\n",
    "\n",
    "\n",
    "# for i, ax in enumerate(axs.flat):\n",
    "#     ax.plot(chosen_parameters[:, i], chosen_parameters[:, chosen_parameters.shape[1] - 1], marker = 'o', linestyle = '')\n",
    "#     ax.set_title(titles[i])\n",
    "#     ax.set_xlabel(titles[i])\n",
    "#     ax.set_xticks(np.unique(chosen_parameters[:, i]))\n",
    "#     ax.grid()\n",
    "\n",
    "# # ax.set_ylabel(\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_parameters_sorted = chosen_parameters[chosen_parameters[:, acc_idx].argsort()]\n",
    "\n",
    "# chosen_parameters_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "times = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN, input_size=[7000, 1, 28, 28], hidden_size = kernel_size_CNN, pool_size1 = pool_size_CNN, pool_size2 = pool_size_CNN)\n",
    "model = model.to(device)\n",
    "optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.eval()\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "model = model.train()\n",
    "start_time = time.time()\n",
    "model = train_model(epochs = 3,\n",
    "                    train_loader = data_train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "elapsed_time = time.time() - start_time\n",
    "times.append(elapsed_time)\n",
    "model = model.eval()\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "test_accuracies.append(trained_acc)\n",
    "print(f\"Loss {trained_loss:.4f} | Accuracy: {trained_acc:.4f}\")\n",
    "\n",
    "# model = model.eval()\n",
    "labels_CNN = get_predicted_labels(test_data = x_test, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average time: {np.mean(times):.2f} s | Max time: {np.max(times):.2f} s | Min time: {np.min(times):.2f} s\")\n",
    "print(f\"Average accuracy: {np.mean(test_accuracies):.2f}% | Max accuracy: {np.max(test_accuracies):.2f}% | Min accuracy: {np.min(test_accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2-layer Fully Convolutional Network\n",
    "\n",
    "class FullyConv_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, hidden_size):\n",
    "        super(FullyConv_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=self._dimensions[1], kernel_size=self._kernel)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConv_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size = 250)\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "start_time = time.time()\n",
    "model = train_model(epochs = 2,\n",
    "                    train_loader = data_train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "elapsed_time = time.time() - start_time\n",
    "model = model.eval()\n",
    "\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "print(f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_subset_scaled.shape)\n",
    "print(data_pca_sigmoid.shape)\n",
    "print(label_predict_SVC_linear.shape)\n",
    "print(label_predict_SVC_sigmoid.shape)\n",
    "print(label_predict_SVC_rbf.shape)\n",
    "print(labels_FCNN.shape)\n",
    "print(labels_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets 30 samples for the specified class according to given labels\n",
    "\n",
    "\n",
    "def show_class(class_number, labels):\n",
    "    indeces = []\n",
    "    for i in range(30):\n",
    "        while labels[i] != class_number:\n",
    "            i = np.random.choice(len(labels), size=1, replace=False)[0]\n",
    "        indeces.append(i)\n",
    "\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(5, 3))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indeces):\n",
    "        axs[i].imshow(\n",
    "            train_subset_scaled[idx].reshape(28, 28), cmap=\"gray\"\n",
    "        )  # Assuming images are 28x28 pixels\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(labels_FCNN))\n",
    "print(set(labels_CNN))\n",
    "print(set(label_predict_SVC_rbf))\n",
    "print(set(label_predict_SVC_linear))\n",
    "print(set(label_predict_SVC_sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"Showing class \" + str(i) + \":\")\n",
    "    show_class(i, labels_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: A *fully-supervised* approach\n",
    "\n",
    "Repeat the steps of *Section 3* using the true labels of the dataset. Comment on the results, and draw a comparison between such results and those obtained from the previous *hybrid* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset as before, but this time use the true labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels_train, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_linear = classifier.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"rbf\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"poly\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_poly = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"sigmoid\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_sigmoid = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: Linear {accuracy_score(y_test, label_predict_SVC_linear):.2} | RBF {accuracy_score(y_test, label_predict_SVC_rbf):.2} | Poly {accuracy_score(y_test, label_predict_SVC_poly):.2} |Sigmoid {accuracy_score(y_test, label_predict_SVC_sigmoid):.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_SVC_true = np.concatenate((y_test.reshape(len(y_test), 1),\n",
    "                             label_predict_SVC_linear.reshape(len(label_predict_SVC_linear), 1),\n",
    "                             label_predict_SVC_rbf.reshape(len(label_predict_SVC_rbf), 1),\n",
    "                             label_predict_SVC_poly.reshape(len(label_predict_SVC_poly), 1),\n",
    "                             label_predict_SVC_sigmoid.reshape(len(label_predict_SVC_sigmoid), 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelPCA(n_components = 2, kernel = \"sigmoid\").fit_transform(x_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize = (15, 3), sharex=True, sharey=True)\n",
    "\n",
    "titles = [\"True labels\", \"Linear kernel\", \"RBF kernel\", \"Polynomial kernel\", \"Sigmoid kernel\"]\n",
    "\n",
    "i = 0\n",
    "for ax in axs:\n",
    "    ax.scatter(model[:, 0], model[:, 1], c = labels_SVC_true[:, i], cmap = 'viridis')\n",
    "    ax.title.set_text(titles[i])\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.hstack((labels_SVC_unsupervised, labels_SVC_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Fully Connected NN\n",
    "Trying different numbers of layers and hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train.reshape(len(y_train)), dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test.reshape(len(y_test)), dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Fully Connected Neural Network with 2 layers and calculate accuracy on the test set for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 250).to(device)\n",
    "    \n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    trained_acc_FC_2l = []\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        \n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 250).to(device)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_FC_2l)\n",
    "    plt.xticks(epochs[::2])\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(50, 100)\n",
    "    # plt.savefig(\"Report/ex5_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    neurons = np.arange(50, 10050, 1000)\n",
    "\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "    trained_acc_FC_2l = []\n",
    "\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i]).to(device)\n",
    "        \n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = 8,\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "    # print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons, trained_acc_FC_2l)\n",
    "    plt.xlabel(\"Number neurons per hidden layer\")\n",
    "    plt.ylabel(\"Trained accuracy\")\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "    trained_acc_FC_2l = []\n",
    "\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 50).to(device)\n",
    "        \n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "    # print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_FC_2l)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Trained accuracy\")\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 50).to(device)\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = 10,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    model = model.eval()\n",
    "            \n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss} | Time: {elapsed_time}\")\n",
    "    accuracies.append(trained_acc)\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "    # labels_FC_2l = get_predicted_labels(test_data = x_test, device = device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: Max {np.max(accuracies):.2f} | Min {np.min(accuracies):.2f} | Mean {np.mean(accuracies):.2f}\")\n",
    "print(f\"Time: Max {np.max(times):.2f} | Min {np.min(times):.2f} | Mean {np.mean(times):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3:\n",
    "Convolutional and Fully Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons per hidden layer\n",
    "if train_multiple_models:\n",
    "    \n",
    "    neurons = np.arange(50, 550, 100)\n",
    "    \n",
    "    labels_CNN_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "    \n",
    "    trained_acc_CNN_2l = []\n",
    "    \n",
    "    for i in range(len(neurons)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN, input_size = [7000, 1, 28, 28], hidden_size = neurons[i], pool_size1 = pool_size_CNN, pool_size2 = pool_size_CNN)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "        model = model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = 3,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "\n",
    "        model = model.eval()\n",
    "        times = time.time() - start_time\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Neurons: {neurons[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "        labels_CNN_2l[:, i] = get_predicted_labels(test_data = x_test, model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons per hidden layer\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons, trained_acc_CNN_2l)\n",
    "    plt.xlabel(\"Number neurons per hidden layer\")\n",
    "    plt.ylabel(\"Trained accuracy\")\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_CNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons per hidden layer\n",
    "if train_multiple_models:\n",
    "    \n",
    "    epochs = np.arange(1, 8, 2)\n",
    "    \n",
    "    labels_CNN_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "    \n",
    "    trained_acc_CNN_2l = []\n",
    "    \n",
    "    for i in range(len(epochs)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN, input_size = [7000, 1, 28, 28], hidden_size = hidden_size_CNN, pool_size1 = kernel_size_CNN, pool_size2 = kernel_size_CNN)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "        model = model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "\n",
    "        model = model.eval()\n",
    "        times = time.time() - start_time\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Epoch: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "        labels_CNN_2l[:, i] = get_predicted_labels(test_data = x_test, model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_CNN_2l)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Trained accuracy\")\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_CNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one set of parameters\n",
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    model = CNN_2layer(n_classes = 10, kernel_size1 = kernel_size_CNN, kernel_size2 = kernel_size_CNN, input_size = [7000, 1, 28, 28], hidden_size = hidden_size_CNN, pool_size1 = pool_size_CNN, pool_size2 = pool_size_CNN)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = learning_rate_CNN)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = epochs_CNN,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    model = model.eval()\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss} | Elapsed time: {elapsed_time:.2f} s\")\n",
    "\n",
    "    accuracies.append(trained_acc)\n",
    "    times.append(elapsed_time)\n",
    "    # model = model.eval()\n",
    "    # labels_CNN_2l = get_predicted_labels(test_data = x_test, model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: Max {np.max(accuracies):.2f} | Min {np.min(accuracies):.2f} | Mean {np.mean(accuracies):.2f}\")\n",
    "print(f\"Time: Max {np.max(times):.2f} | Min {np.min(times):.2f} | Mean {np.mean(times):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
