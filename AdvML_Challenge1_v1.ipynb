{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Challenge 1*: A **kernel** methods / **DL** pipeline for the FashionMNIST dataset\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_Challenge_1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, accuracy_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Used to save data into files\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# Used to measure time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import train dataset only, scale them and convert to data loader\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(0, 1)]),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "print(f\"train_dataset:\\n {train_dataset}\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the original training dataset (composed of 60000 elements) and keep only 10000 of them. Of these, 7000 will be the training dataset, and 3000 will be the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 10000\n",
    "test_percentage = 0.3\n",
    "\n",
    "## set a seed for randperm\n",
    "th.manual_seed(42)\n",
    "idx = th.randperm(len(train_dataset))[: subset_size]\n",
    "print(len(idx))\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "train_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "\n",
    "test_subset_loader = DataLoader(train_dataset, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the images and their labels to numpy arrays and reshape them to vectors\n",
    "labels = np.array([])\n",
    "images = np.array([])\n",
    "for batch in train_subset_loader:\n",
    "    data, label = batch\n",
    "    images = np.append(images, data.numpy().reshape(1, -1))\n",
    "    labels = np.append(labels, label.numpy())\n",
    "\n",
    "train_subset_scaled = images.reshape(subset_size, -1)\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    train_subset_scaled, labels, test_size=0.3, random_state=42, shuffle=False\n",
    ")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of labels for better understanding\n",
    "description = {0: \"T-shirt/top\", \n",
    "               1: \"Trouser\", \n",
    "               2: \"Pullover\", \n",
    "               3: \"Dress\", \n",
    "               4: \"Coat\", \n",
    "               5: \"Sandal\", \n",
    "               6: \"Shirt\", \n",
    "               7: \"Sneaker\", \n",
    "               8: \"Bag\", \n",
    "               9: \"Ankle boot\"}\n",
    "\n",
    "ticks = list(description.keys())\n",
    "tick_labels = list(description.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions to save and load data from pickle files\n",
    "\n",
    "def save_data(data, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pkl.dump(data, f)\n",
    "\n",
    "def load_data(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing samples and checking labels\n",
    "indices = np.random.choice(x_train.shape[0], size=30, replace=False)\n",
    "\n",
    "def print_img_index(indices, img_set, lab_set, class_number):\n",
    "    counter = np.zeros(10)\n",
    "\n",
    "    # Counting number of appearance of each label\n",
    "    for i in range(len(indices)):\n",
    "        label = lab_set[indices[i]]\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Plotting the images with the labels as titles\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        axs[i].imshow(\n",
    "            img_set[idx].reshape(28, 28), cmap=\"gray\"\n",
    "        )  # Assuming images are 28x28 pixels\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner visualization\n",
    "        # string = str(description[labels_subset[idx]])\n",
    "        axs[i].set_title(description[lab_set[idx].item()])\n",
    "\n",
    "    fig.suptitle(f\"Class {class_number}\")\n",
    "\n",
    "    # Printing the counted amounts\n",
    "    for j in range(10):\n",
    "        print(f\"{description[j]}: {counter[j]}\")\n",
    "\n",
    "\n",
    "print_img_index(indices, x_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the color map for the plots\n",
    "colors_rgb = [\n",
    "    (33, 240, 182),\n",
    "    (21, 122, 72),\n",
    "    (155, 209, 198),\n",
    "    (16, 85, 138),\n",
    "    (172, 139, 248),\n",
    "    (133, 22, 87),\n",
    "    (197, 81, 220),\n",
    "    (56, 181, 252),\n",
    "    (18, 85, 211),\n",
    "    (171, 230, 91),\n",
    "]\n",
    "colors_rgb_normalized = colors_rgb_normalized = np.array(colors_rgb) / 255.0\n",
    "cmap = ListedColormap(colors_rgb_normalized)\n",
    "plt.rcParams[\"ps.useafm\"] = True\n",
    "title_dict = {\n",
    "    \"fontname\": \"Sans-serif\",\n",
    "    \"fontsize\": 16,\n",
    "    \"fontweight\": \"bold\",\n",
    "}\n",
    "rcParams.update({\"figure.autolayout\": True})\n",
    "\n",
    "bar_color = (16, 85, 138)\n",
    "bar_rgb_color = np.array(bar_color) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform linear PCA\n",
    "\n",
    "model = PCA(n_components = 3)\n",
    "data_pca_linear = model.fit_transform(x_train, y_train)\n",
    "\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first two principal components\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_linear[:, 0], data_pca_linear[:, 1], c=y_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "# legend = ax.legend(*p.legend_elements(), loc=\"right\", title=\"Classes\")\n",
    "# ax.set_xlim(left=None,right=13)\n",
    "# ax.add_artist(legend)\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "# plt.title(\"First 2 components of linear PCA\", fontdict=title_dict)\n",
    "plt.savefig(\"Report/pca_linear_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del (p, fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first three principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_linear[:, 0],\n",
    "        data_pca_linear[:, 1],\n",
    "        data_pca_linear[:, 2],\n",
    "        c=y_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z) \n",
    "\n",
    "# legend = ax.legend(*p.legend_elements(), loc=\"right\", title=\"Classes\")\n",
    "# ax.add_artist(legend)\n",
    "ax.dist = 13\n",
    "###plt.show()\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p\n",
    "\n",
    "plt.savefig(\"Report/pca_linear_3comps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "The data does not seem to be well separated, so finding the right hyperplane for classification will be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Perform kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel\n",
    "    \n",
    "kernel_pca = KernelPCA(kernel=\"rbf\", n_components = 3)\n",
    "data_pca_rbf = kernel_pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(\n",
    "    data_pca_rbf[:, 0], data_pca_rbf[:, 1], c=y_train, marker=\".\", cmap=cmap\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=11)\n",
    "plt.ylabel(\"Principal \\n Component 2\", fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"Report/pca_rbf_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del (p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_rbf[:, 0],\n",
    "        data_pca_rbf[:, 1],\n",
    "        data_pca_rbf[:, 2],\n",
    "        c=y_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "ax.dist = 13\n",
    "###plt.show()\n",
    "\n",
    "plt.savefig(\"Report/pca_rbf_3comps.png\")\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = 5 / 784)\n",
    "data_pca_rbf = kernel_pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel, tune gamma to separate clusters\n",
    "    \n",
    "gamma = np.array([(1/10)*(1/784), 1/784, 10 * (1/784)])\n",
    "\n",
    "data_pca_rbf = np.ndarray((int(subset_size*(1-test_percentage)), 3, len(gamma)))\n",
    "\n",
    "for i in range(len(gamma)):\n",
    "    kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = gamma[i])\n",
    "    data_pca_rbf[:, :, i] = kernel_pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for different gammas\n",
    "\n",
    "gammas = [\"$\\\\frac{1}{5} * \\\\frac{1}{784}$\", \"$\\\\frac{1}{784}$\", \"$5 * \\\\frac{1}{784}$\"]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(17, 6))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    p = ax.scatter(\n",
    "        data_pca_rbf[:, 0, i], data_pca_rbf[:, 1, i], c=y_train, marker=\".\", cmap=cmap\n",
    "    )\n",
    "    ax.set_title(\"Gamma = \" + gammas[i])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the range of the parameter gamma\n",
    "gammas = np.arange(1/784 - 5 * (1/784) * (1/10), 1/784 + 5 * (1/784) * (1/10), (1/784) * (1/10))\n",
    "\n",
    "## Extract eigenvalues\n",
    "n_components = 3\n",
    "eigenvalues_rbf = np.empty((len(gammas), n_components))\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    kernel_pca = KernelPCA(kernel=\"rbf\", n_components = n_components, gamma = gammas[i])\n",
    "    eigenvalues_rbf[i] = kernel_pca.fit(x_train).eigenvalues_\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(30, 10))\n",
    "# Create 10 random plots\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    x = np.arange(1, len(eigenvalues_rbf[i, :]) + 1, 1)\n",
    "    # Plot the data on the corresponding axis\n",
    "    ax.bar(x, eigenvalues_rbf[i, :], color = bar_rgb_color)\n",
    "    ax.set_ylim(0, 500)\n",
    "    ax.set_xticks(range(1, 4))\n",
    "    # ax.set_xlabel('Component')\n",
    "    # ax.set_ylabel('Eigenvalue')\n",
    "    ax.set_title('Gamma = ' + str(np.round(gammas[i], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Perform kPCA using another kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel poly\n",
    "\n",
    "kernel_pca = KernelPCA(kernel = \"poly\", n_components = 3)\n",
    "\n",
    "data_pca_poly = kernel_pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(data_pca_poly[:, 0], data_pca_poly[:, 1], c = y_train, marker='.', cmap = cmap)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal \\n Component 2', fontsize=11, rotation=0, labelpad=50)\n",
    "\n",
    "plt.savefig(\"Report/pca_poly_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del(p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_poly[:, 0],\n",
    "        data_pca_poly[:, 1],\n",
    "        data_pca_poly[:, 2],\n",
    "        c=y_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "ax.dist = 13\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/pca_poly_3comps.png\")\n",
    "\n",
    "del (p, fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel sigmoid\n",
    "kernel_pca = KernelPCA(kernel=\"sigmoid\", n_components = 10)\n",
    "\n",
    "data_pca_sigmoid = kernel_pca.fit_transform(x_train)\n",
    "\n",
    "eigenvalues_sigmoid = kernel_pca.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "p = plt.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = y_train, marker='.', cmap = cmap)\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal \\n Component 2', fontsize=11, rotation=0, labelpad=50)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.savefig(\"Report/pca_sigmoid_2comps.png\")\n",
    "##plt.show()\n",
    "\n",
    "del(p, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9), dpi=200)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(\n",
    "        data_pca_sigmoid[:, 0],\n",
    "        data_pca_sigmoid[:, 1],\n",
    "        data_pca_sigmoid[:, 2],\n",
    "        c=y_train,\n",
    "        marker=\".\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.yaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.xaxis.set_rotate_label(False)  # disable automatic rotation\n",
    "ax.set_xlabel(\"Principal \\n Component 1\", fontsize=11, labelpad=15)\n",
    "ax.set_ylabel(\"Principal \\n Component 2\", fontsize=11, labelpad=15)\n",
    "ax.set_zlabel(\"Principal \\n Component 3\", fontsize=11, labelpad=15)\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "zticks = ax.get_zticks()\n",
    "empty_labels_x = [\"\" for i in range(len(xticks))]\n",
    "empty_labels_y = [\"\" for i in range(len(yticks))]\n",
    "empty_labels_z = [\"\" for i in range(len(zticks))]\n",
    "ax.set_xticks(xticks, empty_labels_x)\n",
    "ax.set_yticks(yticks, empty_labels_y)\n",
    "ax.set_zticks(zticks, empty_labels_z)\n",
    "\n",
    "ax.dist = 13\n",
    "##plt.show()\n",
    "plt.savefig(\"Report/pca_sigmoid_3comps.png\")\n",
    "\n",
    "del fig\n",
    "del ax\n",
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_linear, y_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_rbf, y_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_poly, y_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, y_train.reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: linear: {DB_score[0]:.4f} | rbf: {DB_score[1]:.4f} | poly: {DB_score[2]:.4f} | sigmoid: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with different techniques\n",
    "\n",
    "labels_Kmeans = KMeans(n_clusters = 10, n_init=10).fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Spectral = SpectralClustering(n_clusters = 10, affinity='nearest_neighbors').fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Gaussian = GaussianMixture(n_components = 10).fit(data_pca_sigmoid).predict(data_pca_sigmoid)\n",
    "\n",
    "ex2_labels = np.array([y_train, labels_Kmeans, labels_Spectral, labels_Gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results and compare them with the original clustering\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True, dpi=200)\n",
    "\n",
    "title_names = [\"Original\", \"K Means\", \"Spectral Clustering\", \"Gaussian Mixture\"]\n",
    "\n",
    "for ax, i in zip(axs.flat, range(4)):\n",
    "    ax.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = ex2_labels[i, :], marker='.', cmap = cmap)\n",
    "    ax.set_title(title_names[i], fontweight=\"bold\", fontsize=13)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.savefig(\"Report/unsupervised_clustering.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Adjusted Rand Index\n",
    "\n",
    "ARI = np.empty(3)\n",
    "\n",
    "for i in range(3):\n",
    "    ARI[i] = adjusted_rand_score(ex2_labels[0, :], ex2_labels[i + 1, :])\n",
    "    print(f\"Adjusted Rand Index for {title_names[i + 1]}: {ARI[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[0, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[1, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[2, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[3, :].reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: original: {DB_score[0]:.4f} | KMeans: {DB_score[1]:.4f} | Spectral: {DB_score[2]:.4f} | Gaussian: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "As we can see, label assignment performed poorly. This, probably, because the clusters are very close to each other and not clearly separated.\n",
    "\n",
    "#### b\n",
    "As we can see from the plot below, there is a clear elbow on the third component. This suggests that 10 does not reflect the actual knee point of the spectrum of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalues obtained with the sigmoid method\n",
    "\n",
    "plt.figure(figsize=(9, 6), dpi=200)\n",
    "plt.plot(np.arange(1, len(eigenvalues_sigmoid) + 1, 1), eigenvalues_sigmoid)\n",
    "\n",
    "plt.xticks(np.arange(1, 11, 1))\n",
    "\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Eigenvalue', rotation=0, labelpad=20)\n",
    "##plt.show()\n",
    "# plt.savefig(\"Report/eigenvalues_sigmoid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized plot of the eigenvalues\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "plt.bar(\n",
    "    x=np.arange(1, len(eigenvalues_sigmoid) + 1),\n",
    "    height=eigenvalues_sigmoid,\n",
    "    color=bar_rgb_color,\n",
    ")\n",
    "xticks = np.arange(1, len(eigenvalues_sigmoid) + 1, 1)\n",
    "plt.xticks(xticks)\n",
    "plt.xlabel(\"Component\", fontsize=11)\n",
    "plt.ylabel(\"Eigenvalue\",  fontsize=11, rotation=0, labelpad=35)\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/eigenvalues_sigmoid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized plot of the eigenvalues\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "plt.bar(\n",
    "    x=np.arange(1, len(eigenvalues_sigmoid) + 1),\n",
    "    height=eigenvalues_sigmoid / np.max(eigenvalues_sigmoid),\n",
    "    color=bar_rgb_color,\n",
    ")\n",
    "xticks = np.arange(1, len(eigenvalues_sigmoid) + 1, 1)\n",
    "plt.xticks(xticks)\n",
    "plt.xlabel(\"Component\", fontsize=11)\n",
    "plt.ylabel(\"Eigenvalue\", fontsize=11, rotation=0, labelpad=35)\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/eigenvalues_sigmoid_normalized.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ARI)\n",
    "del(eigenvalues_sigmoid)\n",
    "del(labels_Kmeans)\n",
    "del(labels_Gaussian)\n",
    "del(ax)\n",
    "del(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sigmoid kernel PCA again to obtain 10000 elements\n",
    "kernel_pca = KernelPCA(kernel=\"sigmoid\", n_components=3)\n",
    "\n",
    "data_pca_sigmoid = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "print(f\"train_subset_scaled.shape = {train_subset_scaled.shape}\")\n",
    "\n",
    "print(f\"data_pca_sigmoid.shape = {data_pca_sigmoid.shape}\")\n",
    "\n",
    "# Now that we have the total 10000 elements of shape 3 again,\n",
    "# use the trained model to predict 10000 labels and store in labels_Gaussian\n",
    "\n",
    "labels_Spectral = (\n",
    "    SpectralClustering(n_clusters=10, affinity=\"nearest_neighbors\")\n",
    "    .fit(data_pca_sigmoid)\n",
    "    .labels_\n",
    ")\n",
    "print(f\"labels_Spectral.shape = {labels_Spectral.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_labels(lab1, lab2):\n",
    "    for i in range(len(lab1)):\n",
    "        if lab1[i] != lab2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def compare_image_arrays(img_array1, img_array2):\n",
    "    for i in range(len(img_array1)):\n",
    "        img1 = img_array1[i]\n",
    "        img2 = img_array2[i]\n",
    "        for j in range(len(img1)):\n",
    "            if not np.array_equal(img1[j], img2[j]):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "# First, get all 10000 (both training and test data) predictions with trained model\n",
    "\n",
    "x_train_ex3, x_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(\n",
    "    train_subset_scaled, labels_Spectral, test_size=0.3, random_state=42, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_img_index(indices, img_set, lab_set, class_number):\n",
    "    # print_img_index(indices, x_test_ex3, true_labels, class_number)\n",
    "    counter = np.zeros(10)\n",
    "\n",
    "    # Counting number of appearance of each label\n",
    "    for i in range(len(indices)):\n",
    "        label = lab_set[indices[i]]\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Plotting the images with the labels as titles\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        axs[i].imshow(\n",
    "            img_set[idx].reshape(28, 28), cmap=\"gray\"\n",
    "        )  # Assuming images are 28x28 pixels\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner visualization\n",
    "        # string = str(description[labels_subset[idx]])\n",
    "        axs[i].set_title(description[lab_set[idx].item()])\n",
    "        # axs[i].set_title(lab_set[idx].item())\n",
    "\n",
    "    fig.suptitle(f\"Class {class_number}\")\n",
    "\n",
    "    # Printing the counted amounts\n",
    "    for j in range(10):\n",
    "        print(f\"{description[j]}: {counter[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets 30 samples for the specified class according to given labels\n",
    "def show_class(class_number, learnt_labels, true_labels):\n",
    "    # show_class(i, labels_CNN, y_test)\n",
    "    indices = []\n",
    "    counter = np.zeros(10)\n",
    "    idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "\n",
    "    # Selecting 30 samples with learnt_label = class number\n",
    "    for i in range(30):\n",
    "        idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "        while learnt_labels[idx] != class_number:\n",
    "            idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "\n",
    "        indices.append(idx)\n",
    "        counter[true_labels[idx]] += 1\n",
    "\n",
    "    # Now that the indices are selected, print the images and the corresponding label on which the model was trained\n",
    "    print_img_index(\n",
    "        indices=indices,\n",
    "        img_set=x_test_ex3,\n",
    "        lab_set=true_labels,\n",
    "        class_number=class_number,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"\\nClass {i}:\")\n",
    "    show_class(class_number=i, learnt_labels=y_test_ex3, true_labels=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: kernel SVM with different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel\n",
    "classifier = SVC(kernel=\"linear\").fit(x_train_ex3, y_train_ex3)\n",
    "\n",
    "label_predict_SVC_linear = classifier.predict(x_test_ex3)\n",
    "acc_linear = accuracy_score(y_test_ex3, label_predict_SVC_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel\n",
    "\n",
    "classifier = SVC(kernel=\"rbf\").fit(x_train_ex3, y_train_ex3)\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test_ex3)\n",
    "acc_rbf = accuracy_score(y_test_ex3, label_predict_SVC_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel\n",
    "\n",
    "classifier = SVC(kernel=\"poly\").fit(x_train_ex3, y_train_ex3.reshape(len(y_train_ex3)))\n",
    "\n",
    "label_predict_SVC_poly = classifier.predict(x_test_ex3)\n",
    "\n",
    "acc_poly = accuracy_score(y_test_ex3, label_predict_SVC_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid kernel\n",
    "\n",
    "classifier = SVC(kernel=\"sigmoid\").fit(\n",
    "    x_train_ex3, y_train_ex3\n",
    ")\n",
    "\n",
    "label_predict_SVC_sigmoid = classifier.predict(x_test_ex3)\n",
    "\n",
    "acc_sigmoid = accuracy_score(y_test_ex3, label_predict_SVC_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: linear: {acc_linear:.2f} | rbf: {acc_rbf:.2f} | poly: {acc_poly:.2f} | sigmoid: {acc_sigmoid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_SVC_unsupervised = np.concatenate(\n",
    "    (\n",
    "        y_test_ex3.reshape(len(y_test_ex3), 1),\n",
    "        label_predict_SVC_linear.reshape(len(y_test_ex3), 1),\n",
    "        label_predict_SVC_rbf.reshape(len(y_test_ex3), 1),\n",
    "        label_predict_SVC_poly.reshape(len(y_test_ex3), 1),\n",
    "        label_predict_SVC_sigmoid.reshape(len(y_test_ex3), 1),\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelPCA(kernel=\"sigmoid\", n_components=3).fit_transform(x_test_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "titles = [\"Original\", \"Linear\", \"RBF\", \"Polynomial\", \"Sigmoid\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "gs = gridspec.GridSpec(2, 6)  # , width_ratios=[1, 1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Create subplots\n",
    "ax1 = plt.subplot(gs[0, :2])\n",
    "ax2 = plt.subplot(gs[0, 2:4])\n",
    "ax3 = plt.subplot(gs[0, 4:])\n",
    "ax4 = plt.subplot(gs[1, 1:3])\n",
    "ax5 = plt.subplot(gs[1, 3:5])\n",
    "\n",
    "# Plot data in subplots (replace with your actual plotting code)\n",
    "ax1.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 0], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax1.title.set_text(titles[0])\n",
    "\n",
    "ax2.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 1], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax2.title.set_text(titles[1])\n",
    "\n",
    "ax3.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 2], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax3.title.set_text(titles[2])\n",
    "\n",
    "ax4.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 3], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax4.title.set_text(titles[3])\n",
    "\n",
    "ax5.scatter(\n",
    "    model[:, 0], model[:, 1], c=labels_SVC_unsupervised[:, 4], cmap=cmap, marker=\".\"\n",
    ")\n",
    "ax5.title.set_text(titles[4])\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/unsupervised_SVC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3), sharex=True, sharey=True)\n",
    "\n",
    "titles = [\"Original\", \"Linear\", \"RBF\", \"Polynomial\", \"Sigmoid\"]\n",
    "\n",
    "i = 0\n",
    "for ax in axs:\n",
    "    ax.scatter(model[:, 0], model[:, 1], c = labels_SVC_unsupervised[:, i], cmap = cmap, marker='.')\n",
    "    ax.title.set_text(titles[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass data to tensors\n",
    "data_train = TensorDataset(\n",
    "    Tensor(x_train_ex3.reshape(-1, 1, 28, 28)), th.tensor(y_train_ex3, dtype=th.long)\n",
    ")\n",
    "data_train_loader = DataLoader(dataset=data_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_test = TensorDataset(\n",
    "    Tensor(x_test_ex3.reshape(-1, 1, 28, 28)), th.tensor(y_test_ex3, dtype=th.long)\n",
    ")\n",
    "data_test_loader = DataLoader(dataset=data_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide if you want to train multiple models with different hyperparameters\n",
    "train_multiple_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions needed to calculate the accuracy\n",
    "\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function used to train the model\n",
    "\n",
    "def train_model(epochs, train_loader, criterion, optimizer, device, model):\n",
    "    _batch_losses = []\n",
    "    \n",
    "    _model = model\n",
    "    for _ in trange(epochs):\n",
    "        _model = _model.train()\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for _, (_images, _labels) in enumerate(train_loader):\n",
    "            _images = _images.to(device)\n",
    "            _labels = _labels.to(device)\n",
    "\n",
    "            _logits = _model(_images)\n",
    "            _loss = criterion(_logits, _labels)\n",
    "            _batch_losses.append(_loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function used to get labels\n",
    "def get_predicted_labels(model, test_data, device):\n",
    "    test_data_tensor = th.tensor(test_data.reshape(-1, 1, 28, 28))\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    labels = []\n",
    "    with th.no_grad():\n",
    "        for i in range(test_data_tensor.shape[0]):\n",
    "            data = test_data_tensor[i].reshape(1, 1, 28, 28)\n",
    "            pred = model(data.to(device))\n",
    "            labels.append(th.argmax(pred).item())\n",
    "            \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fully Connected Neural Network\n",
    "\n",
    "class FullyConnectedNN_1layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes):\n",
    "        \n",
    "        super(FullyConnectedNN_1layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and calculate accuracy on the test set\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose for which epochs to train the model\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "\n",
    "    # Store the accuracies and predicted labels in two arrays\n",
    "    trained_acc_FC_1l = []\n",
    "    labels_FC_1l = np.ndarray((3000, len(epochs)))\n",
    "    \n",
    "    # Choose the loss  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create a vectors to store the training time (column 1) for each epoch (column 0)\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in epochs:\n",
    "        model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        # Keep track of how much time is required to train the model\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = i,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        times[i - 1, 0] = i\n",
    "        times[i - 1, 1] = end_time - start_time\n",
    "        \n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_1l.append(trained_acc)\n",
    "        \n",
    "        \n",
    "        labels_FC_1l[:, i - 1] = get_predicted_labels(model = model, test_data = x_test_ex3, device = device)\n",
    "\n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i - 1, 1]:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy as a function of the number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        np.arange(1, len(trained_acc_FC_1l) + 1, 1),\n",
    "        trained_acc_FC_1l,\n",
    "        color=bar_rgb_color,\n",
    "    )\n",
    "    plt.xticks(np.arange(1, len(trained_acc_FC_1l) + 1, 2))\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_FCNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "class FullyConnectedNN_2layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes, hidden_features):\n",
    "        \n",
    "        super(FullyConnectedNN_2layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = hidden_features)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = hidden_features,\n",
    "                             out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how accuracy vary with the number of hidden neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose for which numbers of neurons to train the model\n",
    "    neurons = np.arange(50, 10050, 100)\n",
    "\n",
    "    trained_acc_FC_2l_neurons = []\n",
    "    labels_FC_2l = np.ndarray((len(x_test_ex3), len(neurons)))\n",
    "\n",
    "    times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i])\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        model = model.eval()\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "        model = model.train()\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = 8,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i, 0] = neurons[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_2l_neurons.append(trained_acc)\n",
    "        \n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        model = model.eval()\n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test_ex3, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy wrt number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(\n",
    "        np.arange(50, 50 + 100 * len(trained_acc_FC_2l_neurons), 100),\n",
    "        trained_acc_FC_2l_neurons,\n",
    "        color=bar_rgb_color,\n",
    "    )\n",
    "    plt.ylim((50, 100))\n",
    "    plt.xlabel(\"Number of hidden neurons\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "\n",
    "    plt.savefig(\"Report/ex3_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how accuracy varies depending on the number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Define vector to keep all the accuracies, that we will plot\n",
    "    trained_acc_FC_2l = []\n",
    "\n",
    "    # Define an array to keep all the predicted labels\n",
    "    labels_FC_2l = np.ndarray((len(x_test_ex3), len(epochs)))\n",
    "\n",
    "    # Choose loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 850)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i, 0] = epochs[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "        \n",
    "        print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        model = model.eval()\n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test_ex3, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(np.arange(1, 21, 1), trained_acc_FC_2l, color=bar_rgb_color)\n",
    "    plt.xticks(np.arange(1, 21, 2))\n",
    "    plt.ylim((50, 100))\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "\n",
    "    plt.savefig(\"Report/ex3_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type of x_test = {x_test_ex3.dtype}\")\n",
    "print(f\"type of x_test_ex3= {x_test_ex3.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "\n",
    "times = []\n",
    "fcnn_test_accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    model = FullyConnectedNN_2layer(\n",
    "        image_dim=28 * 28, n_classes=10, hidden_features=4000\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(\n",
    "        model, criterion, data_train_loader, device\n",
    "    )\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(\n",
    "        epochs=9,\n",
    "        train_loader=data_train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(\n",
    "        model, criterion, data_test_loader, device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\"\n",
    "    )\n",
    "    times.append(elapsed_time)\n",
    "    fcnn_test_accuracies.append(trained_acc)\n",
    "\n",
    "model = model.eval()\n",
    "labels_FCNN = get_predicted_labels(test_data=np.float32(x_test_ex3), device=device, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Average time: {np.mean(times):.2f} s | Max time: {np.max(times):.2f} s | Min time: {np.min(times):.2f} s\"\n",
    ")\n",
    "print(\n",
    "    f\"Average accuracy: {np.mean(fcnn_test_accuracies):.2f}% | Max accuracy: {np.max(fcnn_test_accuracies):.2f}% | Min accuracy: {np.min(fcnn_test_accuracies):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size):\n",
    "        super(CNN_1layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = self._dimensions[1],\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    model = CNN_1layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28])\n",
    "\n",
    "    # Choose loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "    # Define vector to keep all the accuracies, that we will plot\n",
    "    trained_acc_CNN_1l = []\n",
    "\n",
    "    epochs = [1, 10, 20]\n",
    "\n",
    "    CNN_labels = np.ndarray((len(x_test_ex3), len(epochs)))\n",
    "\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        \n",
    "        times[i - 1, 0] = epochs[i]\n",
    "        times[i - 1, 1] = time.time() - start_time\n",
    "            \n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_1l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test_ex3, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_CNN_1l, color=bar_rgb_color)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=15)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "\n",
    "class CNN_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, hidden_size):\n",
    "        super(CNN_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=self._dimensions[1], kernel_size=self._kernel)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of epochs`\n",
    "\n",
    "if train_multiple_models:\n",
    "    model = CNN_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size=100)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trained_acc_CNN_2l = []\n",
    "    CNN_labels = np.ndarray((len(x_test_ex3), len(epochs)))\n",
    "    times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        times[i, 0] = epochs[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test_ex3, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_CNN_2l, color=bar_rgb_color)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for different number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trained_acc_CNN_2l = []\n",
    "\n",
    "    neurons = [50, 500, 1000]\n",
    "\n",
    "    times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "    CNN_labels = np.ndarray((len(x_test), len(neurons)))\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size = neurons[i])\n",
    "        \n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "        model = model.train()\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = 2,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "        times[i, 0] = neurons[i]\n",
    "        times[i, 1] = time.time() - start_time\n",
    "        model = model.eval()\n",
    "\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test_ex3, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons per hidden layer\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons, trained_acc_CNN_2l, color= bar_rgb_color)\n",
    "    plt.xticks(neurons)\n",
    "    plt.xlabel('Number of neurons per hidden layer')\n",
    "    plt.ylabel('Accuracy', rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex3_CNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "times = []\n",
    "cnn_test_accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    model = CNN_2layer(\n",
    "        n_classes=10, kernel_size=3, input_size=[7000, 1, 28, 28], hidden_size=50\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(\n",
    "        model, criterion, data_train_loader, device\n",
    "    )\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(\n",
    "        epochs=5,\n",
    "        train_loader=data_train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(\n",
    "        model, criterion, data_test_loader, device\n",
    "    )\n",
    "    cnn_test_accuracies.append(trained_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\"\n",
    "    )\n",
    "\n",
    "model = model.eval()\n",
    "labels_CNN = get_predicted_labels(test_data=np.float32(x_test_ex3), device=device, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Average time: {np.mean(times):.2f} s | Max time: {np.max(times):.2f} s | Min time: {np.min(times):.2f} s\"\n",
    ")\n",
    "print(\n",
    "    f\"Average accuracy: {np.mean(cnn_test_accuracies):.2f}% | Max accuracy: {np.max(cnn_test_accuracies):.2f}% | Min accuracy: {np.min(cnn_test_accuracies):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2-layer Fully Convolutional Network\n",
    "\n",
    "class FullyConv_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, hidden_size):\n",
    "        super(FullyConv_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=self._dimensions[1], kernel_size=self._kernel)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConv_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size = 250)\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "start_time = time.time()\n",
    "model = train_model(epochs = 2,\n",
    "                    train_loader = data_train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "elapsed_time = time.time() - start_time\n",
    "model = model.eval()\n",
    "\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "print(f\"Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {elapsed_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Wrap-up!\n",
    "\n",
    "Evaluate the overall accuracy of the pipeline on the *test set* of *FashionMNIST*. *I.e.* compare the predicted labels from the three classifiers built in *Section 3* with the true labels.\n",
    "\n",
    "In order to assign a true label *name* (e.g. *trousers*, *sandal*, ...) to those determined just from *(kernel-)PCA* (that obviously carry no direct information about the subject of the picture), you can either:\n",
    "\n",
    "i. *Cheat* and use the most abundant labels for each group of *(kernel-)PCA-labelled* datapoints.\n",
    "\n",
    "ii. Sample a subset of datapoints from each *(kernel-)PCA-labelled* class, and assign one label by direct visual inspection. If you choose this route, it may also serve as a reminder of the fact that *expert labelling* is not always a trivial (and almost never a fast) task!\n",
    "\n",
    "Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_subset_scaled.shape)\n",
    "print(labels_Spectral.shape)\n",
    "print(data_pca_sigmoid.shape)\n",
    "print(label_predict_SVC_linear.shape)\n",
    "print(label_predict_SVC_sigmoid.shape)\n",
    "print(label_predict_SVC_rbf.shape)\n",
    "print(labels_FCNN.shape)\n",
    "print(labels_CNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and testing datasets. Also printing an image and its label. This is to remind us that the labels are given by the Spectral clustering and therefore can be wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the labels are taken from the Spectral clustering!!\n",
    "idx = 16\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(x_test[idx].reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "##plt.show()\n",
    "\n",
    "print(description[labels_Spectral[idx].item()])\n",
    "print(x_train.shape)\n",
    "print(train_subset_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_img_index(indices, img_set, lab_set, class_number):\n",
    "    #print_img_index(indices, x_test_ex3, true_labels, class_number)\n",
    "    counter = np.zeros(10)\n",
    "\n",
    "    # Counting number of appearance of each label\n",
    "    for i in range(len(indices)):\n",
    "        label = lab_set[indices[i]]\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Plotting the images with the labels as titles\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        axs[i].imshow(\n",
    "            img_set[idx].reshape(28, 28), cmap=\"gray\"\n",
    "        )  # Assuming images are 28x28 pixels\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner visualization\n",
    "        # string = str(description[labels_subset[idx]])\n",
    "        axs[i].set_title(description[lab_set[idx].item()])\n",
    "        #axs[i].set_title(lab_set[idx].item())\n",
    "\n",
    "    fig.suptitle(f\"Class {class_number}\")\n",
    "\n",
    "    # Printing the counted amounts\n",
    "    for j in range(10):\n",
    "        print(f\"{description[j]}: {counter[j]}\")\n",
    "    \n",
    "    return np.argmax(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets 30 samples for the specified class according to given labels\n",
    "def show_class(class_number, learnt_labels, true_labels):\n",
    "    # show_class(i, labels_CNN, y_test)\n",
    "    indices = []\n",
    "    counter = np.zeros(10)\n",
    "    idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "\n",
    "    # Selecting 30 samples with learnt_label = class number\n",
    "    for i in range(30):\n",
    "        idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "        while learnt_labels[idx] != class_number:\n",
    "            idx = np.random.choice(len(learnt_labels), size=1, replace=False)[0]\n",
    "\n",
    "        indices.append(idx)\n",
    "        counter[true_labels[idx]] += 1\n",
    "\n",
    "    # Now that the indices are selected, print the images and the corresponding label on which the model was trained\n",
    "    physical_class = print_img_index(\n",
    "        indices=indices,\n",
    "        img_set=x_test_ex3,\n",
    "        lab_set=true_labels,\n",
    "        class_number=class_number,\n",
    "    )\n",
    "    return physical_class.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_classes = np.zeros(10)\n",
    "for i in range(10):\n",
    "    print(f\"\\nClass {i}:\")\n",
    "    physical_classes[i] = show_class(\n",
    "        class_number=i, learnt_labels=labels_CNN, true_labels=y_test\n",
    "    )\n",
    "\n",
    "physical_classes = physical_classes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from one set of 10 integer numbers to another set of 10 integer numbers\n",
    "pred_class = [i for i in range(10)]\n",
    "class_mapping = dict(zip(pred_class, physical_classes))\n",
    "\n",
    "# Example usage:\n",
    "input_value = 4\n",
    "output_value = class_mapping.get(input_value)\n",
    "\n",
    "if output_value is not None:\n",
    "    print(f\"The mapped value for {input_value} is {description[output_value]}\")\n",
    "else:\n",
    "    print(f\"No mapping found for {input_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(physical_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.array([])\n",
    "for element in labels_CNN:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_CNN = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"CNN overall accuracy: {overall_accuracy_CNN:.2f}\")\n",
    "del predicted_labels\n",
    "\n",
    "predicted_labels = np.array([])\n",
    "for element in labels_FCNN:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_FCNN = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"FCNN overall accuracy: {overall_accuracy_FCNN:.2f}\")\n",
    "del predicted_labels\n",
    "\n",
    "predicted_labels = np.array([])\n",
    "for element in label_predict_SVC_linear:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_linear = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"Linear SVM overall accuracy: {overall_accuracy_linear:.2f}\")\n",
    "\n",
    "predicted_labels = np.array([])\n",
    "for element in label_predict_SVC_rbf:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_rbf = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"RBF SVM overall accuracy: {overall_accuracy_rbf:.2f}\")\n",
    "\n",
    "predicted_labels = np.array([])\n",
    "for element in label_predict_SVC_poly:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_poly = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"Polynomial SVM overall accuracy: {overall_accuracy_poly:.2f}\")\n",
    "\n",
    "predicted_labels = np.array([])\n",
    "for element in label_predict_SVC_sigmoid:\n",
    "    predicted_labels = np.append(predicted_labels, class_mapping.get(element))\n",
    "overall_accuracy_sigmoid = accuracy_score(y_true=y_test, y_pred=predicted_labels)\n",
    "print(f\"Sigmoid SVM overall accuracy: {overall_accuracy_sigmoid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart\n",
    "import pandas as pd\n",
    "\n",
    "methods = [\"SVC linear\", \"SVC RBF\", \"SVC Polynomial\", \"SVC sigmoid\", \"FCNN\", \"CNN\"]\n",
    "accuracies = np.round(\n",
    "    [\n",
    "        100 * overall_accuracy_linear,\n",
    "        100 * overall_accuracy_rbf,\n",
    "        100 * overall_accuracy_poly,\n",
    "        100 * overall_accuracy_sigmoid,\n",
    "        100 * overall_accuracy_FCNN,\n",
    "        100 * overall_accuracy_CNN\n",
    "    ],\n",
    "    2,\n",
    ")\n",
    "print(accuracies)\n",
    "df = pd.DataFrame({\"methods\": methods, \"accuracies\": accuracies})\n",
    "df.sort_values(\"accuracies\", inplace=True, ascending=False)\n",
    "\n",
    "print(df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.barh(y=df.methods, width=df.accuracies, color = bar_rgb_color)\n",
    "ax.set_yticks(np.arange(len(df.methods)))\n",
    "ax.set_xlabel(\"Accuracy (%)\")\n",
    "ax.invert_yaxis()\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width(),\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{bar.get_width():.2f}\",\n",
    "        va=\"center\",\n",
    "        ha=\"right\",\n",
    "        fontsize=8,\n",
    "        color=\"white\",\n",
    "    )\n",
    "\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/ex3_overall_accuracies.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: A *fully-supervised* approach\n",
    "\n",
    "Repeat the steps of *Section 3* using the true labels of the dataset. Comment on the results, and draw a comparison between such results and those obtained from the previous *hybrid* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset as before, but this time use the true labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels, test_size = 0.3, random_state = 42, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_linear = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"rbf\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"poly\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_poly = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"sigmoid\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict_SVC_sigmoid = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: Linear {accuracy_score(y_test, label_predict_SVC_linear):.2} | RBF {accuracy_score(y_test, label_predict_SVC_rbf):.2} | Poly {accuracy_score(y_test, label_predict_SVC_poly):.2} |Sigmoid {accuracy_score(y_test, label_predict_SVC_sigmoid):.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_SVC_true = np.concatenate((y_test.reshape(len(y_test), 1),\n",
    "                             label_predict_SVC_linear.reshape(len(label_predict_SVC_linear), 1),\n",
    "                             label_predict_SVC_rbf.reshape(len(label_predict_SVC_rbf), 1),\n",
    "                             label_predict_SVC_poly.reshape(len(label_predict_SVC_poly), 1),\n",
    "                             label_predict_SVC_sigmoid.reshape(len(label_predict_SVC_sigmoid), 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelPCA(n_components=2, kernel=\"sigmoid\").fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"True labels\", \"Linear\", \"RBF\", \"Polynomial\", \"Sigmoid\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), dpi=200)\n",
    "gs = gridspec.GridSpec(2, 6)  # , width_ratios=[1, 1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Create subplots\n",
    "ax1 = plt.subplot(gs[0, :2])\n",
    "ax2 = plt.subplot(gs[0, 2:4])\n",
    "ax3 = plt.subplot(gs[0, 4:])\n",
    "ax4 = plt.subplot(gs[1, 1:3])\n",
    "ax5 = plt.subplot(gs[1, 3:5])\n",
    "\n",
    "# Plot data in subplots (replace with your actual plotting code)\n",
    "ax1.scatter(model[:, 0], model[:, 1], c=labels_SVC_true[:, 0], cmap=cmap, marker=\".\")\n",
    "ax1.title.set_text(titles[0])\n",
    "\n",
    "ax2.scatter(model[:, 0], model[:, 1], c=labels_SVC_true[:, 1], cmap=cmap, marker=\".\")\n",
    "ax2.title.set_text(titles[1])\n",
    "\n",
    "ax3.scatter(model[:, 0], model[:, 1], c=labels_SVC_true[:, 2], cmap=cmap, marker=\".\")\n",
    "ax3.title.set_text(titles[2])\n",
    "\n",
    "ax4.scatter(model[:, 0], model[:, 1], c=labels_SVC_true[:, 3], cmap=cmap, marker=\".\")\n",
    "ax4.title.set_text(titles[3])\n",
    "\n",
    "ax5.scatter(model[:, 0], model[:, 1], c=labels_SVC_true[:, 4], cmap=cmap, marker=\".\")\n",
    "ax5.title.set_text(titles[4])\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "##plt.show()\n",
    "\n",
    "plt.savefig(\"Report/unsupervised_SVC_true_labels.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KernelPCA(n_components = 2, kernel = \"sigmoid\").fit_transform(x_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize = (15, 3), sharex=True, sharey=True)\n",
    "\n",
    "titles = [\"True labels\", \"Linear kernel\", \"RBF kernel\", \"Polynomial kernel\", \"Sigmoid kernel\"]\n",
    "\n",
    "i = 0\n",
    "for ax in axs:\n",
    "    ax.scatter(model[:, 0], model[:, 1], c = labels_SVC_true[:, i], cmap = cmap)\n",
    "    ax.title.set_text(titles[i])\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.hstack((labels_SVC_unsupervised, labels_SVC_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "title = [\"Unsupervised\"]\n",
    "fig, axs = plt.subplots(3, 3, figsize = (9, 9), sharex=True, sharey=True)\n",
    "for ax in axs:\n",
    "    if i == 0 or i == 5:\n",
    "        continue\n",
    "    else:\n",
    "        ax.scatter(model[:, 0], model[:, 1], c = labels[:, i], cmap = cmap)\n",
    "        ax.title.set_text(titles[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Fully Connected NN\n",
    "Trying different numbers of layers and hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train.reshape(len(y_train)), dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test.reshape(len(y_test)), dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Fully Connected Neural Network with 1 layer and calculate accuracy on the test set for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    epochs = np.arange(1, 21, 1)\n",
    "\n",
    "    trained_acc_FC_1l = []\n",
    "    labels_FC_1l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        \n",
    "        model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_1l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_1l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_FC_1l, color=bar_rgb_color)\n",
    "    plt.xticks(epochs[::2])\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_FCNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Fully Connected Neural Network with 2 layers and calculate accuracy on the test set for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features=250).to(device)\n",
    "\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    trained_acc_FC_2l = []\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "    for i in range(len(epochs)):\n",
    "        \n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features=250).to(device)\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, trained_acc_FC_2l, color=bar_rgb_color)\n",
    "    plt.xticks(epochs[::2])\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Choose the loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    neurons = np.arange(50, 10050, 100)\n",
    "\n",
    "    labels_FC_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "    trained_acc_FC_2l = []\n",
    "\n",
    "\n",
    "    for i in range(len(neurons)):\n",
    "        model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i]).to(device)\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "        \n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = 2,\n",
    "                                train_loader = data_train_loader,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer,\n",
    "                                device = device,\n",
    "                                model = model)\n",
    "        model = model.eval()\n",
    "                \n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        \n",
    "        trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "        \n",
    "        labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "    # print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons, trained_acc_FC_2l, color=bar_rgb_color)\n",
    "    plt.xlabel(\"Number neurons per hidden layer\")\n",
    "    plt.ylabel(\"Trained accuracy\")\n",
    "    plt.ylim(50, 100, rotation=0, labelpad=20)\n",
    "    plt.savefig(\"Report/ex5_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model and a set of parameter to predict the labels\n",
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 6000).to(device)\n",
    "optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "model = train_model(epochs = 10,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "model = model.eval()\n",
    "        \n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "\n",
    "labels_FC_2l = get_predicted_labels(test_data = np.float32(x_test), device = device, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3:\n",
    "Convolutional and Fully Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multiple_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 1 layer and calculate accuracy on the test set for different number of epochs\n",
    "\n",
    "if train_multiple_models:\n",
    "    # Device selection\n",
    "    \n",
    "    epochs = np.arange(1, 21, 1)\n",
    "    \n",
    "    trained_acc_CNN_1l = []\n",
    "    labels_CNN_1l = np.ndarray((len(x_test), len(epochs)))\n",
    "    \n",
    "    for i in range(len(epochs)):\n",
    "        model = CNN_1layer(n_classes = 10, kernel_size = 3, input_size = [7000, 1, 28, 28])\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "        optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "        \n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "        \n",
    "        print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "        model = model.train()\n",
    "\n",
    "        model = train_model(epochs = epochs[i],\n",
    "                                    train_loader = data_train_loader,\n",
    "                                    criterion = criterion,\n",
    "                                    optimizer = optimizer,\n",
    "                                    device = device,\n",
    "                                    model = model)\n",
    "        model = model.eval()\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        trained_acc_CNN_1l.append(trained_acc)\n",
    "        \n",
    "        print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "\n",
    "        labels_CNN_1l[:, i] = get_predicted_labels(test_data= np.float32(x_test), model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "if train_multiple_models:\n",
    "    plt.plot(epochs, np.array(trained_acc_CNN_1l), color=bar_rgb_color)\n",
    "    plt.xticks(epochs[::2])\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_CNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Convolutional Neural Network with 2 layers and calculate accuracy on the test set for different number of neurons per hidden layer\n",
    "if train_multiple_models:\n",
    "    \n",
    "    neurons = np.arange(50, 550, 100)\n",
    "    \n",
    "    labels_CNN_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "    \n",
    "    trained_acc_CNN_2l = []\n",
    "    \n",
    "    for i in range(len(neurons)):\n",
    "        model = CNN_2layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28], hidden_size = neurons[i])\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        model = model.eval()\n",
    "\n",
    "        untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "        model = model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        model = train_model(epochs = 3,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "\n",
    "        model = model.eval()\n",
    "        times = time.time() - start_time\n",
    "        trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "        print(f\"Neurons: {neurons[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times:.2f} s\")\n",
    "        \n",
    "        trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "        model = model.eval()\n",
    "        labels_CNN_2l[:, i] = get_predicted_labels(test_data = np.float32(x_test), model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons per hidden layer\n",
    "\n",
    "if train_multiple_models:\n",
    "    plt.plot(neurons, trained_acc_CNN_2l, color=bar_rgb_color)\n",
    "    plt.xlabel(\"Number neurons per hidden layer\")\n",
    "    plt.ylabel(\"Trained accuracy\", rotation=0, labelpad=20)\n",
    "    plt.ylim(50, 100)\n",
    "    plt.savefig(\"Report/ex5_CNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one set of parameters\n",
    "\n",
    "model = CNN_2layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28], hidden_size = 200)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "model = train_model(epochs = 3,\n",
    "                    train_loader = data_train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "\n",
    "model = model.eval()\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "\n",
    "model = model.eval()\n",
    "labels_CNN_2l = get_predicted_labels(test_data = np.float32(x_test), model = model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
