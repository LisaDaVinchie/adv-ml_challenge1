{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Challenge 1*: A **kernel** methods / **DL** pipeline for the FashionMNIST dataset\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_Challenge_1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, accuracy_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Used to save data into files\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# Used to measure time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import train and test dataset, scale them and convert them to data loaders\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select some images from the training and test dataset\n",
    "\n",
    "subset_size = 10000\n",
    "\n",
    "## set a seed for randperm\n",
    "th.manual_seed(42)\n",
    "\n",
    "idx = th.randperm(len(train_dataset))[:subset_size]\n",
    "\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "\n",
    "train_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "\n",
    "idx = th.randperm(len(test_dataset))[:subset_size]\n",
    "\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "\n",
    "test_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "\n",
    "del(idx)\n",
    "del(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the images and their labels to numpy arrays and reshape them to vectors\n",
    "\n",
    "labels_train = []\n",
    "train_subset = []\n",
    "for batch in train_subset_loader:\n",
    "    data, labels = batch\n",
    "    \n",
    "    train_subset.append(data.numpy().reshape(1, -1))\n",
    "    labels_train.append(labels.numpy())\n",
    "\n",
    "train_subset_scaled = np.array(train_subset).reshape(subset_size, -1)\n",
    "labels_train = np.array(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = []\n",
    "labels_test = []\n",
    "\n",
    "for batch in test_subset_loader:\n",
    "    data, labels = batch\n",
    "    \n",
    "    test_subset.append(data.numpy().reshape(1, -1))\n",
    "    labels_test.append(labels.numpy())\n",
    "\n",
    "test_subset_scaled = np.array(test_subset).reshape(subset_size, -1)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of labels for better understanding\n",
    "description = {0: \"T-shirt/top\", \n",
    "               1: \"Trouser\", \n",
    "               2: \"Pullover\", \n",
    "               3: \"Dress\", \n",
    "               4: \"Coat\", \n",
    "               5: \"Sandal\", \n",
    "               6: \"Shirt\", \n",
    "               7: \"Sneaker\", \n",
    "               8: \"Bag\", \n",
    "               9: \"Ankle boot\"}\n",
    "\n",
    "ticks = list(description.keys())\n",
    "tick_labels = list(description.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions to save and load data from pickle files\n",
    "\n",
    "def save_data(data, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pkl.dump(data, f)\n",
    "\n",
    "def load_data(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the color map for the plots\n",
    "cmap = plt.get_cmap('viridis', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform linear PCA\n",
    "\n",
    "model = PCA(n_components = 3)\n",
    "data_pca_linear = model.fit_transform(train_subset_scaled)\n",
    "\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first two principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "p = plt.scatter(data_pca_linear[:, 0], data_pca_linear[:, 1], c = labels_train, marker='.', cmap = cmap)\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "\n",
    "\n",
    "# Uncomment to plot the colorbar\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.set_ticks(ticks)\n",
    "# cb.set_ticklabels(tick_labels)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "# plt.savefig(\"Report/pca_linear_2comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first two principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(data_pca_linear[:, 0], data_pca_linear[:, 1], data_pca_linear[:, 2], c = labels_train, marker='.', cmap=cmap)\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "\n",
    "# Uncomment to show colorbar\n",
    "cb = plt.colorbar(p, ax = ax, shrink = 1, aspect = 20)\n",
    "cb.set_ticks(ticks)\n",
    "cb.set_ticklabels(tick_labels, fontsize = 11)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "del(cb)\n",
    "\n",
    "del(fig)\n",
    "del(ax)\n",
    "del(p)\n",
    "\n",
    "\n",
    "# plt.savefig(\"Report/pca_linear_3comps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "The data does not seem to be well separated, so finding the right hyperplane for classification will be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Perform kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel\n",
    "    \n",
    "kernel_pca = KernelPCA(kernel=\"rbf\", n_components = 3)\n",
    "data_pca_rbf = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "p = plt.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], c = labels_train, marker='.', cmap=cmap)\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "del(p)\n",
    "del(fig)\n",
    "# plt.savefig(\"Report/pca_rbf_2comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], data_pca_rbf[:, 2], c = labels_train, marker='.', cmap=cmap)\n",
    "\n",
    "cb = plt.colorbar(p, ax = ax, shrink = 1, aspect = 20)\n",
    "cb.set_ticks(ticks)\n",
    "cb.set_ticklabels(tick_labels, fontsize = 11)\n",
    "del(cb)\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "del(fig)\n",
    "del(ax)\n",
    "del(p)\n",
    "\n",
    "# plt.savefig(\"Report/pca_rbf_3comps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = 5 / 784)\n",
    "data_pca_rbf = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "plt.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], c = labels_train, marker='.', cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kernel pca using the RBF kernel, tune gamma to separate clusters\n",
    "    \n",
    "gamma = np.array([(1/10)*(1/784), 1/784, 10 * (1/784)])\n",
    "\n",
    "data_pca_rbf = np.ndarray((10000, 3, len(gamma)))\n",
    "\n",
    "for i in range(len(gamma)):\n",
    "    kernel_pca = KernelPCA(kernel = \"rbf\", n_components = 3, gamma = gamma[i])\n",
    "    data_pca_rbf[:, :, i] = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for different gammas\n",
    "\n",
    "gammas = [\"$\\\\frac{1}{5} * \\\\frac{1}{784}$\", \"$\\\\frac{1}{784}$\", \"$5 * \\\\frac{1}{784}$\"]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    p = ax.scatter(data_pca_rbf[:, 0, i], data_pca_rbf[:, 1, i], c = labels_train, marker='.', cmap=cmap)\n",
    "    ax.set_title('Gamma = ' + gammas[i])\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the range of the parameter gamma\n",
    "gammas = np.arange(1/784 - 5 * (1/784) * (1/10), 1/784 + 5 * (1/784) * (1/10), (1/784) * (1/10))\n",
    "\n",
    "## Extract eigenvalues\n",
    "n_components = 3\n",
    "eigenvalues_rbf = np.empty((len(gammas), n_components))\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    kernel_pca = KernelPCA(kernel=\"rbf\", n_components = n_components, gamma = gammas[i])\n",
    "    eigenvalues_rbf[i] = kernel_pca.fit(train_subset_scaled).eigenvalues_\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(30, 10))\n",
    "# Create 10 random plots\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    x = np.arange(1, len(eigenvalues_rbf[i, :]) + 1, 1)\n",
    "    # Plot the data on the corresponding axis\n",
    "    ax.bar(x, eigenvalues_rbf[i, :])\n",
    "    ax.set_ylim(0, 1000)\n",
    "    # ax.set_xlabel('Component')\n",
    "    # ax.set_ylabel('Eigenvalue')\n",
    "    ax.set_title('Gamma = ' + str(gammas[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Perform kPCA using another kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel poly\n",
    "\n",
    "kernel_pca = KernelPCA(kernel = \"poly\", n_components = 3)\n",
    "\n",
    "data_pca_poly = kernel_pca.fit_transform(train_subset_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "p = plt.scatter(data_pca_poly[:, 0], data_pca_poly[:, 1], c = labels_train, marker='.', cmap = cmap)\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "\n",
    "# plt.savefig(\"Report/pca_poly_2comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(data_pca_poly[:, 0], data_pca_poly[:, 1], data_pca_poly[:, 2], c = labels_train, marker='.', cmap=cmap)\n",
    "\n",
    "cb = plt.colorbar(p, ax = ax, shrink = 1, aspect = 20)\n",
    "cb.set_ticks(ticks)\n",
    "cb.set_ticklabels(tick_labels, fontsize = 11)\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "# plt.savefig(\"Report/pca_poly_3comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try kernel sigmoid\n",
    "kernel_pca = KernelPCA(kernel=\"sigmoid\", n_components = 10)\n",
    "\n",
    "data_pca_sigmoid = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "eigenvalues_sigmoid = kernel_pca.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "p = plt.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = labels_train, marker='.', cmap = cmap)\n",
    "\n",
    "# cb = plt.colorbar(p)\n",
    "# cb.ax.set_title('Class', fontsize=11)\n",
    "# del(cb)\n",
    "\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "\n",
    "# plt.savefig(\"Report/pca_sigmoid_2comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 principal components\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(3):\n",
    "    p = ax.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], data_pca_sigmoid[:, 2], c = labels_train, marker='.', cmap = cmap)\n",
    "\n",
    "cb = plt.colorbar(p, ax = ax, shrink = 1, aspect = 20)\n",
    "cb.set_ticks(ticks)\n",
    "cb.set_ticklabels(tick_labels, fontsize = 11)\n",
    "\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "# plt.savefig(\"Report/pca_sigmoid_3comps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_linear, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_rbf, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_poly, labels_train.reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels_train.reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: linear: {DB_score[0]:.4f} | rbf: {DB_score[1]:.4f} | poly: {DB_score[2]:.4f} | sigmoid: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(p)\n",
    "del(axs)\n",
    "del(ax)\n",
    "del(fig)\n",
    "del(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with different techniques\n",
    "\n",
    "labels_Kmeans = KMeans(n_clusters = 10, n_init=10).fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Spectral = SpectralClustering(n_clusters = 10, affinity='nearest_neighbors').fit(data_pca_sigmoid).labels_\n",
    "\n",
    "labels_Gaussian = GaussianMixture(n_components = 10).fit(data_pca_sigmoid).predict(data_pca_sigmoid)\n",
    "\n",
    "labels = np.array([labels_train.reshape(subset_size), labels_Kmeans, labels_Spectral, labels_Gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results and compare them with the original clustering\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "title_names = [\"Original\", \"K Means\", \"Spectral Clustering\", \"Gaussian Mixture\"]\n",
    "\n",
    "for ax, i in zip(axs.flat, range(4)):\n",
    "    ax.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = labels[i, :], marker='.')\n",
    "    ax.set_title(title_names[i])  \n",
    "# plt.savefig(\"Report/unsupervised_clustering.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Adjusted Rand Index\n",
    "\n",
    "ARI = np.empty(3)\n",
    "\n",
    "for i in range(3):\n",
    "    ARI[i] = adjusted_rand_score(labels[0, :], labels[i + 1, :])\n",
    "    print(f\"Adjusted Rand Index for {title_names[i + 1]}: {ARI[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure separation of the clusters using the Davies-Bouldin score\n",
    "# The lower the better\n",
    "\n",
    "# DB_score = []\n",
    "\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[0, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[1, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[2, :].reshape(-1)))\n",
    "# DB_score.append(davies_bouldin_score(data_pca_sigmoid, labels[3, :].reshape(-1)))\n",
    "\n",
    "# print(f\"DB score: original: {DB_score[0]:.4f} | KMeans: {DB_score[1]:.4f} | Spectral: {DB_score[2]:.4f} | Gaussian: {DB_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "As we can see, label assignment performed poorly. This, probably, because the clusters are very close to each other and not clearly separated.\n",
    "\n",
    "#### b\n",
    "As we can see from the plot below, there is a clear elbow on the third component. This suggests that 10 does not reflect the actual knee point of the spectrum of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalues obtained with the sigmoid method\n",
    "\n",
    "plt.plot(np.arange(1, len(eigenvalues_sigmoid) + 1, 1), eigenvalues_sigmoid)\n",
    "\n",
    "plt.xticks(np.arange(1, 11, 1))\n",
    "\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "# plt.savefig(\"Report/eigenvalues_sigmoid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ARI)\n",
    "del(eigenvalues_sigmoid)\n",
    "del(labels_Kmeans)\n",
    "del(labels_Gaussian)\n",
    "del(ax)\n",
    "del(axs)\n",
    "del(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels_Spectral, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: kernel SVM with different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel\n",
    "\n",
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train)\n",
    "\n",
    "label_predict_SVC_linear = classifier.predict(x_test)\n",
    "\n",
    "acc_linear = accuracy_score(y_test, label_predict_SVC_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel\n",
    "\n",
    "classifier = SVC(kernel = \"rbf\").fit(x_train, y_train)\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test)\n",
    "\n",
    "acc_rbf = accuracy_score(y_test, label_predict_SVC_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel\n",
    "\n",
    "classifier = SVC(kernel = \"poly\").fit(x_train, y_train)\n",
    "\n",
    "label_predict_SVC_rbf = classifier.predict(x_test)\n",
    "\n",
    "acc_poly = accuracy_score(y_test, label_predict_SVC_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid kernel\n",
    "\n",
    "classifier = SVC(kernel = \"sigmoid\").fit(x_train, y_train)\n",
    "\n",
    "label_predict_SVC_sigmoid = classifier.predict(x_test)\n",
    "\n",
    "acc_sigmoid = accuracy_score(y_test, label_predict_SVC_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: linear: {acc_linear:.4f} | rbf: {acc_rbf:.4f} | poly: {acc_poly:.4f} | sigmoid: {acc_sigmoid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass data to tensors\n",
    "\n",
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train, dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test, dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions needed to calculate the accuracy\n",
    "\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function used to train the model\n",
    "\n",
    "def train_model(epochs, train_loader, criterion, optimizer, device, model):\n",
    "    _batch_losses = []\n",
    "    \n",
    "    _model = model\n",
    "    for _ in trange(epochs):\n",
    "        _model = _model.train()\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for _, (_images, _labels) in enumerate(train_loader):\n",
    "            _images = _images.to(device)\n",
    "            _labels = _labels.to(device)\n",
    "\n",
    "            _logits = _model(_images)\n",
    "            _loss = criterion(_logits, _labels)\n",
    "            _batch_losses.append(_loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function used to get labels\n",
    "def get_predicted_labels(model, test_data, device):\n",
    "    test_data_tensor = th.tensor(test_data.reshape(-1, 1, 28, 28))\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    labels = []\n",
    "    with th.no_grad():\n",
    "        for i in range(test_data_tensor.shape[0]):\n",
    "            data = test_data_tensor[i].reshape(1, 1, 28, 28)\n",
    "            pred = model(data.to(device))\n",
    "            labels.append(th.argmax(pred).item())\n",
    "            \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fully Connected Neural Network\n",
    "\n",
    "class FullyConnectedNN_1layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes):\n",
    "        \n",
    "        super(FullyConnectedNN_1layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and calculate accuracy on the test set\n",
    "\n",
    "# Choose for which epochs to train the model\n",
    "epochs = np.arange(1, 21, 1)\n",
    "\n",
    "# Store the accuracies and predicted labels in two arrays\n",
    "trained_acc_FC_1l = []\n",
    "labels_FC_1l = np.ndarray((3000, len(epochs)))\n",
    "  \n",
    "# Choose the loss  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a vectors to store the training time (column 1) for each epoch (column 0)\n",
    "times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "for i in epochs:\n",
    "    model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "\n",
    "    model = model.train()\n",
    "    \n",
    "    # Keep track of how much time is required to train the model\n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = i,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    times[i - 1, 0] = i\n",
    "    times[i - 1, 1] = end_time - start_time\n",
    "    \n",
    "    model = model.eval()\n",
    "            \n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    trained_acc_FC_1l.append(trained_acc)\n",
    "    \n",
    "    \n",
    "    labels_FC_1l[:, i - 1] = get_predicted_labels(model = model, test_data = x_test, device = device)\n",
    "\n",
    "    print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i - 1, 1]:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy as a function of the number of epochs\n",
    "\n",
    "plt.plot(np.arange(1, len(trained_acc_FC_1l) + 1, 1), trained_acc_FC_1l)\n",
    "plt.xticks(np.arange(1, len(trained_acc_FC_1l) + 1, 2))\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex3_FCNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "\n",
    "class FullyConnectedNN_2layer(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes, hidden_features):\n",
    "        \n",
    "        super(FullyConnectedNN_2layer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = image_dim,\n",
    "                            out_features = hidden_features)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = hidden_features,\n",
    "                             out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how accuracy vary with the number of hidden neurons\n",
    "\n",
    "# Choose for which numbers of neurons to train the model\n",
    "neurons = np.arange(50, 10050, 100)\n",
    "\n",
    "trained_acc_FC_2l_neurons = []\n",
    "labels_FC_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "\n",
    "times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(len(neurons)):\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "        \n",
    "    model = model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = 8,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    \n",
    "    times[i, 0] = neurons[i]\n",
    "    times[i, 1] = time.time() - start_time\n",
    "    \n",
    "            \n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    trained_acc_FC_2l_neurons.append(trained_acc)\n",
    "    \n",
    "    print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "    \n",
    "    model = model.eval()\n",
    "    labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy wrt number of neurons\n",
    "\n",
    "plt.plot(np.arange(50, 50 + 100 * len(trained_acc_FC_2l_neurons), 100), trained_acc_FC_2l_neurons)\n",
    "plt.ylim((50, 100))\n",
    "plt.xlabel(\"Number of hidden neurons\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# plt.savefig(\"Report/ex3_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how accuracy varies depending on the number of epochs\n",
    "\n",
    "# Define vector to keep all the accuracies, that we will plot\n",
    "trained_acc_FC_2l = []\n",
    "\n",
    "# Define an array to keep all the predicted labels\n",
    "labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "# Choose loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = 850)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, data_train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = epochs[i],\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    \n",
    "    times[i, 0] = epochs[i]\n",
    "    times[i, 1] = time.time() - start_time\n",
    "    \n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    trained_acc_FC_2l.append(trained_acc)\n",
    "    \n",
    "    print(f\"Epochs: {i} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "    \n",
    "    model = model.eval()\n",
    "    labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device=device, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "plt.plot(np.arange(1, 21, 1), trained_acc_FC_2l)\n",
    "plt.xticks(np.arange(1, 21, 2))\n",
    "plt.ylim((50, 100))\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# plt.savefig(\"Report/ex3_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size):\n",
    "        super(CNN_1layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = self._dimensions[1],\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "model = CNN_1layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28])\n",
    "\n",
    "# Choose loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define vector to keep all the accuracies, that we will plot\n",
    "trained_acc_CNN_1l = []\n",
    "\n",
    "epochs = [1, 10, 20]\n",
    "\n",
    "CNN_labels = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = epochs[i],\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    \n",
    "    times[i - 1, 0] = epochs[i]\n",
    "    times[i - 1, 1] = time.time() - start_time\n",
    "        \n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "    \n",
    "    trained_acc_CNN_1l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "plt.plot(epochs, trained_acc_CNN_1l)\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex3_CNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with two layers\n",
    "\n",
    "class CNN_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, hidden_size):\n",
    "        super(CNN_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=self._dimensions[1], kernel_size=self._kernel)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CNN_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_acc_CNN_2l = []\n",
    "\n",
    "CNN_labels = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "times = np.ndarray((len(epochs), 2))\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    model = model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = epochs[i],\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    times[i, 0] = epochs[i]\n",
    "    times[i, 1] = time.time() - start_time\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "    \n",
    "    trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "plt.plot(epochs, trained_acc_CNN_2l)\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex3_CNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_acc_CNN_2l = []\n",
    "\n",
    "neurons = [50, 500, 1000]\n",
    "\n",
    "times = np.ndarray((len(neurons), 2))\n",
    "\n",
    "CNN_labels = np.ndarray((len(x_test), len(neurons)))\n",
    "\n",
    "for i in range(len(neurons)):\n",
    "    model = CNN_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size = neurons[i])\n",
    "    \n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    model = model.train()\n",
    "    start_time = time.time()\n",
    "    model = train_model(epochs = 2,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    times[i, 0] = neurons[i]\n",
    "    times[i, 1] = time.time() - start_time\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "    print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")\n",
    "    \n",
    "    trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    CNN_labels[:, i] = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of neurons per hidden layer\n",
    "\n",
    "plt.plot(neurons, trained_acc_CNN_2l)\n",
    "plt.xticks(neurons)\n",
    "plt.xlabel('Number of neurons per hidden layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex3_CNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2-layer Fully Convolutional Network\n",
    "\n",
    "class FullyConv_2layer(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, hidden_size):\n",
    "        super(FullyConv_2layer, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        \n",
    "        self._padding = 0 # Default value\n",
    "        \n",
    "        self._stride = 1 # Default value\n",
    "        \n",
    "        self._stride_inv = 1 / self._stride\n",
    "        \n",
    "        self._kernel = kernel_size\n",
    "        \n",
    "        self._dimensions = input_size\n",
    "        \n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=self._dimensions[1],\n",
    "                               out_channels = hidden_size,\n",
    "                               kernel_size=self._kernel)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "            \n",
    "        self.pool1 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=self._dimensions[1], kernel_size=self._kernel)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, stride = self._stride)\n",
    "        \n",
    "        self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - 2) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - 2) * self._stride_inv + 1\n",
    "                                ]\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConv_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size = 250)\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "model = train_model(epochs = 2,\n",
    "                    train_loader = data_train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "model = model.eval()\n",
    "\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_acc_FCN_2l = []\n",
    "\n",
    "neurons = [50, 100, 500]\n",
    "\n",
    "for i in neurons:\n",
    "    \n",
    "    model = FullyConv_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size=i)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    model = model.train()\n",
    "    model = train_model(epochs = 2,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    trained_acc_FCN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    FCN_labels = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neurons, trained_acc_FCN_2l)\n",
    "plt.xticks(neurons)\n",
    "plt.xlabel(\"Number of neurons per hidden layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.savefig(\"Report/ex3_FCN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FullyConv_2layer(n_classes = 10, kernel_size = 3, input_size=[7000, 1, 28, 28], hidden_size=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_acc_FCN_2l = []\n",
    "\n",
    "epochs = [1, 10, 20]\n",
    "\n",
    "for i in epochs:\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "    model = model.train()\n",
    "    model = train_model(epochs = i,\n",
    "                        train_loader = data_train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    model = model.eval()\n",
    "\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    trained_acc_FCN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    FCN_labels = get_predicted_labels(model = model, test_data = x_test, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy wrt the number of epochs used to train the model\n",
    "\n",
    "plt.plot(epochs, trained_acc_FCN_2l)\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "plt.savefig(\"Report/ex3_FCN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: A *fully-supervised* approach\n",
    "\n",
    "Repeat the steps of *Section 3* using the true labels of the dataset. Comment on the results, and draw a comparison between such results and those obtained from the previous *hybrid* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Importing the dataset again\n",
    "\n",
    "# subset_size = 64\n",
    "\n",
    "# train_dataset = datasets.FashionMNIST(\n",
    "#     root=\"./data\",\n",
    "#     train=True,\n",
    "#     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0, std=1)]),\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# test_dataset = datasets.FashionMNIST(\n",
    "#     root=\"./data\",\n",
    "#     train=False,\n",
    "#     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0, std=1)]),\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(dataset = train_dataset, shuffle=False, batch_size = subset_size)\n",
    "# test_loader = DataLoader(dataset = test_dataset, shuffle=False, batch_size = subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset as before, but this time use the true labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels_train, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict = classifier.predict(x_test)\n",
    "accuracy_score(y_test, label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"rbf\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict = classifier.predict(x_test)\n",
    "accuracy_score(y_test, label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"poly\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict = classifier.predict(x_test)\n",
    "accuracy_score(y_test, label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"sigmoid\").fit(x_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "label_predict = classifier.predict(x_test)\n",
    "accuracy_score(y_test, label_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Fully Connected NN\n",
    "Trying different numbers of layers and hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train.reshape(len(y_train)), dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test.reshape(len(y_test)), dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "epochs = np.arange(1, 21, 1)\n",
    "\n",
    "trained_acc_FC_1l = []\n",
    "labels_FC_1l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    \n",
    "    model = FullyConnectedNN_1layer(image_dim = 28 * 28, n_classes = 10).to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "    model = model.train()\n",
    "\n",
    "    model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "    model = model.eval()\n",
    "            \n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "    trained_acc_FC_1l.append(trained_acc)\n",
    "\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    labels_FC_1l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "# print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, trained_acc_FC_1l)\n",
    "plt.xticks(epochs[::2])\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex5_FCNN1l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features=250).to(device)\n",
    "\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "trained_acc_FC_2l = []\n",
    "labels_FC_2l = np.ndarray((len(x_test), len(epochs)))\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    \n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features=250).to(device)\n",
    "\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "    \n",
    "    model = model.train()\n",
    "\n",
    "    model = train_model(epochs = epochs[i],\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "    model = model.eval()\n",
    "            \n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "    trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "# print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, trained_acc_FC_2l)\n",
    "plt.xticks(epochs[::2])\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex5_FCNN2l_accuracy-epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "neurons = np.arange(50, 10050, 100)\n",
    "\n",
    "labels_FC_2l = np.ndarray((len(x_test), len(neurons)))\n",
    "trained_acc_FC_2l = []\n",
    "\n",
    "\n",
    "for i in range(len(neurons)):\n",
    "    model = FullyConnectedNN_2layer(image_dim = 28 * 28, n_classes = 10, hidden_features = neurons[i]).to(device)\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "    \n",
    "    model = model.train()\n",
    "\n",
    "    model = train_model(epochs = 2,\n",
    "                            train_loader = data_train_loader,\n",
    "                            criterion = criterion,\n",
    "                            optimizer = optimizer,\n",
    "                            device = device,\n",
    "                            model = model)\n",
    "    model = model.eval()\n",
    "            \n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, data_test_loader, device)\n",
    "    \n",
    "    trained_acc_FC_2l.append(trained_acc)\n",
    "\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    labels_FC_2l[:, i] = get_predicted_labels(test_data = x_test, device = device, model = model)\n",
    "\n",
    "# print(f\"Epochs: {epochs[i]} | Accuracy: untrained: {untrained_acc:.4f} | Trained: {trained_acc:.4f} | Time: {times[i, 1]:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neurons, trained_acc_FC_2l)\n",
    "# plt.xticks(neurons[::100])\n",
    "plt.xlabel(\"Number neurons per hidden layer\")\n",
    "plt.ylabel(\"Trained accuracy\")\n",
    "plt.ylim(50, 100)\n",
    "# plt.savefig(\"Report/ex5_FCNN2l_accuracy-neurons.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3: Convolutional NN\n",
    "Again, trying different numbers of layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "model = CNN_1layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "# optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "trained_acc_CNN_1l, trained_loss, model_CNN_1l = train_model(epochs = EPOCHS,\n",
    "                                                             train_loader = data_train_loader,\n",
    "                                                             test_loader = data_test_loader,\n",
    "                                                             criterion = criterion,\n",
    "                                                             optimizer = optimizer,\n",
    "                                                             device = device,\n",
    "                                                             model = model)\n",
    "print(f\"Accuracy: {trained_acc_CNN_1l} | Loss: {trained_loss}\")\n",
    "\n",
    "labels_CNN_1l = get_predicted_labels(test_data=x_test, model = model_CNN_1l, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_CNN_1l).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_2layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28], hidden_size = 100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "model = train_model(epochs = EPOCHS,\n",
    "                    train_loader = train_loader,\n",
    "                    criterion = criterion,\n",
    "                    optimizer = optimizer,\n",
    "                    device = device,\n",
    "                    model = model)\n",
    "\n",
    "model = model.eval()\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "\n",
    "model = model.eval()\n",
    "labels_CNN_2l = get_predicted_labels(test_data=x_test, model = model, device = device)\n",
    "\n",
    "j += 1\n",
    "\n",
    "# del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_acc_CNN_2l = []\n",
    "\n",
    "labels_CNN_2l = np.ndarray((3000, 5))\n",
    "\n",
    "j = 0\n",
    "\n",
    "neurons = np.arange(50, 550, 100)\n",
    "\n",
    "for i in range(len(neurons)):\n",
    "    model = CNN_2layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28], hidden_size = i)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "\n",
    "    # optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "    \n",
    "    model = model.train()\n",
    "\n",
    "    model = train_model(epochs = EPOCHS,\n",
    "                        train_loader = train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    \n",
    "    name = \"CNN_2layer_trained_\" + str(i)\n",
    "    th.save(model, name)\n",
    "    \n",
    "    model = model.eval()\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "    labels_CNN_2l[:, j] = get_predicted_labels(test_data = x_test, model = model, device = device)\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(np.unique(labels_CNN_2l[:, i]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(50, 550, 100), trained_acc_CNN_2l)\n",
    "plt.xlabel(\"Number neurons per hidden layer\")\n",
    "plt.ylabel(\"Trained accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_acc_CNN_2l = []\n",
    "\n",
    "labels_CNN_2l = np.ndarray((3000, 7))\n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in np.arange(1, 7, 1):\n",
    "    model = CNN_2layer(n_classes = 10, kernel_size = 2, input_size = [7000, 1, 28, 28], hidden_size = 50)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "\n",
    "    # optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "    print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "    \n",
    "    model = model.train()\n",
    "\n",
    "    model = train_model(epochs = i,\n",
    "                        train_loader = train_loader,\n",
    "                        criterion = criterion,\n",
    "                        optimizer = optimizer,\n",
    "                        device = device,\n",
    "                        model = model)\n",
    "    \n",
    "    name = \"CNN_2layer_trained_\" + str(i)\n",
    "    th.save(model, name)\n",
    "    \n",
    "    model = model.eval()\n",
    "    trained_loss, trained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "    print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n",
    "    \n",
    "    trained_acc_CNN_2l.append(trained_acc)\n",
    "\n",
    "    model = model.eval()\n",
    "    labels_CNN_2l[:, j] = get_predicted_labels(test_data = x_test, model = model, device = device)\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 7, 1), trained_acc_CNN_2l)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Trained accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
