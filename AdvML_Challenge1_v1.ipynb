{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Challenge 1*: A **kernel** methods / **DL** pipeline for the FashionMNIST dataset\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_Challenge_1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this first *challenge* of the *Advanced Machine Learning Course*, you will experiment with the development of a data analysis pipeline based upon various techniques seen during the lectures so far: some **unsupervised** (*e.g.* *PCA*, *kernel-PCA*) and some others **supervised** (*e.g.* *kernel SVM* and *Artificial Neural Networks* for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Used to save data into files\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The dataset of interest for the *challenge* will be [*FashionMNIST*](https://github.com/zalandoresearch/fashion-mnist), an *MNIST*-like dataset of grayscale images of fashion items. Originally developed by Zalando Research in 2017 as a harder (yet *drop-in* compatible) replacement for the original *MNIST* dataset, it has been used in several papers and competitions since then.\n",
    "\n",
    "Remember to consider the *dataset* mentioned below as the **training set** offered by *FashionMNIST*. Use the *test set* only at the end, to evaluate the overall accuracy of the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import train and test dataset, scale them and convert them to data loaders\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = len(train_dataset),\n",
    "                          shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                          batch_size = len(test_dataset),\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select 10000 images from the training and test dataset\n",
    "subset_size = 10000\n",
    "th.manual_seed(42)\n",
    "idx = th.randperm(len(train_dataset))[:subset_size]\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "train_subset_loader = DataLoader(train_dataset, sampler=sampler)\n",
    "idx = th.randperm(len(test_dataset))[:subset_size]\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "test_subset_loader = DataLoader(train_dataset, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the images and their labels to numpy arrays and reshape them to vectors\n",
    "labels_subset = []\n",
    "train_subset = []\n",
    "for batch in train_subset_loader:\n",
    "    data, labels = batch\n",
    "    train_subset.append(data.numpy().reshape(1, -1))\n",
    "    labels_subset.append(labels.numpy())\n",
    "\n",
    "train_subset_scaled = np.array(train_subset).reshape(subset_size, -1)\n",
    "print(train_subset_scaled.shape)\n",
    "labels_subset = np.array(labels_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = []\n",
    "labels_test = []\n",
    "\n",
    "for batch in test_subset_loader:\n",
    "    data, labels = batch\n",
    "    \n",
    "    test_subset.append(data.numpy().reshape(1, -1))\n",
    "    labels_test.append(labels.numpy())\n",
    "\n",
    "test_subset_scaled = np.array(test_subset).reshape(subset_size, -1)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of labels for better understanding\n",
    "description = {0: \"T-shirt/top\", \n",
    "               1: \"Trouser\", \n",
    "               2: \"Pullover\", \n",
    "               3: \"Dress\", \n",
    "               4: \"Coat\", \n",
    "               5: \"Sandal\", \n",
    "               6: \"Shirt\", \n",
    "               7: \"Sneaker\", \n",
    "               8: \"Bag\", \n",
    "               9: \"Ankle boot\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions to save and load data from pickle files\n",
    "def save_data(data, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pkl.dump(data, f)\n",
    "\n",
    "def load_data(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding data geometry\n",
    "\n",
    "Load the dataset in the most suitable form for the tasks that follow. Then, perform the following steps, with the goal of developing a geometric understanding of the dataset:\n",
    "\n",
    "1. Perform a (linear) *PCA* on the dataset, and plot the first two (or three!) principal components along with the true label. Comment on data separation.\n",
    "\n",
    "2. Perform a *kernel-PCA* on the dataset with a Gaussian kernel, and plot the first two (or three!) principal components along with the true label. Try to tune the dispersion parameter of the kernel to obtain a good separation of the data. Comment.\n",
    "\n",
    "3. Perform another *kernel-PCA* on the dataset with another kernel of your own choice, and plot the first two (or three!) principal components along with the true label. Try to tune the degree of the polynomial kernel to obtain a good separation of the data. Comment.\n",
    "\n",
    "Whenever suitable, try to complement your analysis with some graphs!\n",
    "\n",
    "**IMPORTANT NOTICE**: As some of you have reported, performing *kernel PCA* on the entire *FashionMNIST* dataset can be memory- and time- demanding (as it scales with the square of the number of datapoints!). In case you want to reduce such requirements, you can either:\n",
    "  - Reduce the number of datapoints on which to perform *kPCA*, *e.g.* by slicing the randomly-shuffled dataset *(most effective!)*;\n",
    "  - Reduce the size of the images in the dataset (*i.e.* by dropping even/odd rows/columns, or performing local pooling) *(somehow effective: you may want to try it if you do not have access to powerful compute, but still enjoy challenges!)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform linear PCA and print its first 2 and 3 components\n",
    "# ## Save data to files\n",
    "# if os.path.exists(\"data_pca_linear.pickle\"):\n",
    "#      ## If the file exixts, load it\n",
    "#     data_pca_linear = load_data(\"data_pca_linear.pickle\")\n",
    "    \n",
    "# else:\n",
    "#     ## If the file does not exixts, create it\n",
    "    \n",
    "model = PCA()\n",
    "data_pca_linear = model.fit_transform(train_subset_scaled)\n",
    "# save_data(data_pca_linear, \"data_pca_linear.pickle\")\n",
    "        \n",
    "plt.scatter(data_pca_linear[:, 0], data_pca_linear[:, 1], c = labels_subset, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color the data points based on the labels\n",
    "ax.scatter(data_pca_linear[:, 0], data_pca_linear[:, 1], data_pca_linear[:, 2], c = labels_subset, marker='.')\n",
    "\n",
    "ax.view_init(elev=90, azim=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "The data does not seem to be well separated, so finding the right hyperplane for classification will be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Perform kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform kernel pca using the RBF kernel\n",
    "# if os.path.exists(\"data_pca_rbf.pickle\"):\n",
    "#      ## If the file exixts, load it\n",
    "#     data_pca_rbf = load_data(\"data_pca_rbf.pickle\")\n",
    "    \n",
    "# else:\n",
    "    ## If the file does not exixts, create it\n",
    "    \n",
    "kernel_pca = KernelPCA(kernel=\"rbf\", n_components=15)\n",
    "data_pca_rbf = kernel_pca.fit_transform(train_subset_scaled)\n",
    "\n",
    "# save_data(data_pca_rbf, \"data_pca_rbf.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], c = labels_subset, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color the data points based on the labels\n",
    "ax.scatter(data_pca_rbf[:, 0], data_pca_rbf[:, 1], data_pca_rbf[:, 2], c = labels_subset, marker='.')\n",
    "\n",
    "ax.view_init(elev=0, azim=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the range of the parameter gamma\n",
    "gammas = np.arange(start=1/1000, stop=1/10, step=0.01)\n",
    "print(gammas)\n",
    "\n",
    "if os.path.exists(\"eigenvalues_rbf.pickle\"):\n",
    "     ## If the file exixts, load it\n",
    "    eigenvalues_rbf = load_data(\"eigenvalues_rbf.pickle\")\n",
    "    \n",
    "else:\n",
    "    ## If the file does not exixts, create it\n",
    "\n",
    "    ## Extract eigenvalues\n",
    "    n_components = 3\n",
    "    eigenvalues_rbf = np.empty((len(gammas), n_components))\n",
    "\n",
    "    for i in range(len(gammas)):\n",
    "        kernel_pca = KernelPCA(kernel=\"rbf\", n_components = n_components, gamma = gammas[i])\n",
    "        eigenvalues_rbf[i] = kernel_pca.fit(train_subset_scaled).eigenvalues_\n",
    "        \n",
    "    save_data(eigenvalues_rbf, \"eigenvalues_rbf.pickle\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, 5, figsize=(30, 10))\n",
    "\n",
    "# Create 10 random plots\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Generate random data\n",
    "    x = np.arange(1, len(eigenvalues_rbf[i, :]) + 1, 1)\n",
    "    # Plot the data on the corresponding axis\n",
    "    ax.bar(x, eigenvalues_rbf[i, :])\n",
    "    ax.set_ylim(0, 1000)\n",
    "    # ax.set_xlabel('Component')\n",
    "    # ax.set_ylabel('Eigenvalue')\n",
    "    ax.set_title('Gamma = ' + str(gammas[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Perform kPCA using another kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try kernel poly\n",
    "if os.path.exists(\"data_pca_poly.pickle\"):\n",
    "    data_pca_poly = load_data(\"data_pca_poly.pickle\")  ## If the file exixts, load it\n",
    "\n",
    "else:\n",
    "    ## If the file does not exixts, create it\n",
    "    kernel_pca = KernelPCA(kernel=\"poly\", n_components=3)\n",
    "    data_pca_poly = kernel_pca.fit_transform(train_subset_scaled)\n",
    "    save_data(data_pca_poly, \"data_pca_poly.pickle\")\n",
    "\n",
    "plt.scatter(data_pca_poly[:, 0], data_pca_poly[:, 1], c=labels_subset, marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color the data points based on the labels\n",
    "ax.scatter(data_pca_poly[:, 0], data_pca_poly[:, 1], data_pca_poly[:, 2], c = labels_subset, marker='.')\n",
    "\n",
    "ax.view_init(elev=45, azim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try kernel sigmoid\n",
    "\n",
    "# if os.path.exists(\"data_pca_sigmoid.pickle\") and os.path.exists(\"eigenvalues_sigmoid.pickle\"):\n",
    "    \n",
    "#      ## If the file exixts, load it\n",
    "#     data_pca_sigmoid = load_data(\"data_pca_sigmoid.pickle\")\n",
    "#     eigenvalues_sigmoid = load_data(\"eigenvalues_sigmoid.pickle\")\n",
    "    \n",
    "# else:\n",
    "    ## If the file does not exixts, create it\n",
    "    \n",
    "kernel_pca = KernelPCA(kernel=\"sigmoid\", n_components = 10)\n",
    "data_pca_sigmoid = kernel_pca.fit_transform(train_subset_scaled)\n",
    "# save_data(data_pca_sigmoid, \"data_pca_sigmoid.pickle\")\n",
    "eigenvalues_sigmoid = kernel_pca.eigenvalues_\n",
    "# save_data(eigenvalues_sigmoid, \"eigenvalues_sigmoid.pickle\")\n",
    "plt.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = labels_subset, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color the data points based on the labels\n",
    "ax.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], data_pca_sigmoid[:, 2], c = labels_subset, marker='.')\n",
    "\n",
    "ax.view_init(elev=0, azim=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Bridging *unsupervised* and *supervised*\n",
    "\n",
    "Choose one of the results obtained in the previous section (you should choose the one better explaining data geometry), and ignore the true labels. Then, perform the following steps:\n",
    "\n",
    "1. Considering only the first $10$ components of the *(kernel-)PCA* and try to assign $10$ labels to the resulting datapoints. Choose the approach you deem most suitable. Comment on the results, by considering:\n",
    "    \n",
    "    a. How well does the label-assignment just performed reflect the true labels?\n",
    "\n",
    "    b. Does the number of components used ($10$) reflect the actual *knee-* or *gap-* *point* of the spectrum associated to the principal components?\n",
    "\n",
    "Whenever suitable, try to complement your analysis with some graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_Kmeans = KMeans(n_clusters = 10).fit(data_pca_sigmoid).labels_\n",
    "labels_Spectral = SpectralClustering(n_clusters = 10, affinity='nearest_neighbors').fit(data_pca_sigmoid).labels_\n",
    "labels_Gaussian = GaussianMixture(n_components = 10).fit(data_pca_sigmoid).predict(data_pca_sigmoid)\n",
    "labels = np.array([labels_subset.reshape(subset_size), labels_Kmeans, labels_Spectral, labels_Gaussian])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "title_names = [\"Original\", \"KMeans\", \"Spectral Clustering\", \"Gaussian Mixture\"]\n",
    "\n",
    "for ax, i in zip(axs.flat, range(4)):\n",
    "    ax.scatter(data_pca_sigmoid[:, 0], data_pca_sigmoid[:, 1], c = labels[i, :], marker='.')\n",
    "    ax.set_title(title_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjusted Rand Index\n",
    "ARI = np.empty(3)\n",
    "\n",
    "for i in range(3):\n",
    "    ARI[i] = adjusted_rand_score(labels[0, :], labels[i + 1, :])\n",
    "    print(\"Adjusted Rand Index for \" + title_names[i + 1] + \": \" + str(ARI[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a\n",
    "As we can see, label assignment performed poorly. This, probably, because the clusters are very close to each other and not clearly separated.\n",
    "\n",
    "#### b\n",
    "As we can see from the plot below, there is a clear elbow on the third component. This suggests that 10 does not reflect the actual knee point of the spectrum of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, len(eigenvalues_sigmoid) + 1, 1), eigenvalues_sigmoid)\n",
    "\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: (Supervised) classification\n",
    "\n",
    "Consider the dataset composed of the original images, with the label assigned in the previous section (regardless of its actual match with the true label!). Then, define and learn a classifier that can predict the label of a new image. Specifically:\n",
    "\n",
    "1. Learn a *kernel-SVM* on the data/label pairs. The choice of the kernel and its hyperparameters is up to your experimentation and time availability. Comment on your choices and results.\n",
    "\n",
    "2. Learn a *fully-connected NN* on the data/label pairs. The choice of the architecture and its hyperparameters is up to your experimentation and time availability: show at least two different hyperparameter configurations, and comment on the results.\n",
    "\n",
    "3. Learn a *CNN* on the data/label pairs. The choice of the architecture and its hyperparameters is up to your experimentation and time availability. Comment on the results with special respect to the FCN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_subset_scaled, labels_Gaussian, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "fcnn_model = FCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "\n",
    "## listen to lecture ~11:39\n",
    "device = th.device(device=\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Re-instantiate the model to reset the weights/gradients. Weights are initialized by default according to Kaiming He et al. (2015) | https://arxiv.org/abs/1502.01852\n",
    "fcnn_model: FCNN = fcnn_model.to(device=device)  # Move the model to the selected device\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = th.optim.SGD(fcnn_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    fcnn_model = fcnn_model.train()\n",
    "\n",
    "    # Actual (batch-wise) training step\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = fcnn_model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        batch_losses.append(loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    fcnn_model.eval()\n",
    "\n",
    "# Saving weights\n",
    "dirname = \"models/\"\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)\n",
    "th.save(fcnn_model.state_dict(), dirname + \"rotation_invariant_slfcn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: kernel SVM with different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\").fit(x_train, y_train)\n",
    "labels_predict = classifier.predict(x_test)\n",
    "acc = accuracy_score(y_test, labels_predict)\n",
    "print(\"Accuracy score for linear kernel SVM: \" + str(acc))\n",
    "\n",
    "SVM_predicted_labels = labels_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel=\"sigmoid\").fit(x_train, y_train)\n",
    "labels_predict = classifier.predict(x_test)\n",
    "acc2 = accuracy_score(y_test, labels_predict)\n",
    "\n",
    "print(\"Accuracy score for sigmoid kernel SVM: \" + str(acc2))\n",
    "if acc2 > acc:\n",
    "    acc = acc2\n",
    "    SVM_predicted_labels = labels_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel=\"rbf\").fit(x_train, y_train)\n",
    "labels_predict = classifier.predict(x_test)\n",
    "acc2 = accuracy_score(y_test, labels_predict)\n",
    "\n",
    "print(\"Accuracy score for rbf kernel SVM: \" + str(acc2))\n",
    "\n",
    "if acc2 > acc:\n",
    "    acc = acc2\n",
    "    SVM_predicted_labels = labels_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we use the rbf kernel for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass data to tensors\n",
    "data_train = TensorDataset(Tensor(x_train.reshape(-1, 1, 28, 28)), th.tensor(y_train, dtype = th.long))\n",
    "data_train_loader = DataLoader(dataset = data_train, batch_size = 64, shuffle = True)\n",
    "\n",
    "data_test = TensorDataset(Tensor(x_test.reshape(-1, 1, 28, 28)), th.tensor(y_test, dtype = th.long))\n",
    "data_test_loader = DataLoader(dataset = data_test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions needed to calculate the accuracy\n",
    "def get_batch_accuracy(logit, target):\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def get_test_stats(model, criterion, test_loader, device):\n",
    "    test_acc, test_loss = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        test_acc += get_batch_accuracy(outputs, labels)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function used to train the model\n",
    "\n",
    "def train_model(epochs, train_loader, test_loader, criterion, optimizer, device, model):\n",
    "    _batch_losses = []\n",
    "\n",
    "    for _ in trange(epochs):\n",
    "        _model = model.train()\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for _, (_images, _labels) in enumerate(train_loader):\n",
    "            _images = _images.to(device)\n",
    "            _labels = _labels.to(device)\n",
    "\n",
    "            _logits = _model(_images)\n",
    "            _loss = criterion(_logits, _labels)\n",
    "\n",
    "            _batch_losses.append(_loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _model.eval()\n",
    "        \n",
    "    _trained_loss, _trained_acc = get_test_stats(_model, criterion, test_loader, device)\n",
    "\n",
    "    print(f\"Accuracy: {_trained_acc} | Loss: {_trained_loss}\")\n",
    "    \n",
    "    return _trained_acc, _trained_loss, _model, _batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fully Connected Neural Network\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, image_dim, n_classes):\n",
    "        \n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = image_dim,\n",
    "                            out_features = n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise model\n",
    "model_FCNN = FullyConnectedNN(image_dim = 28 * 28, n_classes = 10)\n",
    "\n",
    "## Choose optimizer\n",
    "optimizer = th.optim.SGD(model_FCNN.parameters(), lr=0.0001, momentum=0)\n",
    "\n",
    "# Choose loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Device selection\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model_FCNN = model_FCNN.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Untrained Accurac\n",
    "model_FCNN = model_FCNN.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model = model_FCNN,\n",
    "                                               criterion = criterion,\n",
    "                                               test_loader = data_test_loader,\n",
    "                                               device = device)\n",
    "\n",
    "model_FCNN = model_FCNN.train()\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trained_acc, trained_loss, model_FCNN, batch_losses = train_model(epochs = 2,\n",
    "                                                             train_loader = data_train_loader,\n",
    "                                                             test_loader = data_test_loader,\n",
    "                                                             criterion = criterion,\n",
    "                                                             optimizer = optimizer,\n",
    "                                                             device = device,\n",
    "                                                             model = model_FCNN)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size, input_size, layers, pooling):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self._n_classes = n_classes\n",
    "        self._padding = 0 # Default value\n",
    "        self._stride = 1 # Default value\n",
    "        self._stride_inv = 1 / self._stride\n",
    "        self._kernel = kernel_size\n",
    "        self._dimensions = input_size\n",
    "        self._print_dimensions = False\n",
    "        self._layers = layers\n",
    "        self._pooling = pooling\n",
    "        \n",
    "        if self._layers >= 1:   \n",
    "            self.conv1, self.bn1, self._dimensions = self.conv_layer(self._kernel)\n",
    "         \n",
    "        if self._layers >= 2:\n",
    "            self.conv2, self.bn2, self._dimensions = self.conv_layer(self._kernel)\n",
    "            \n",
    "        if self._pooling == True:\n",
    "            self.pool = nn.MaxPool2d(kernel_size = self._kernel, stride = self._stride)\n",
    "            if self._print_dimensions == True:\n",
    "                print(\"Defined pool\")\n",
    "        \n",
    "            self._dimensions = [self._dimensions[0],\n",
    "                                self._dimensions[1] ,\n",
    "                                (self._dimensions[2] - self._kernel) * self._stride_inv + 1,\n",
    "                                (self._dimensions[3] - self._kernel) * self._stride_inv + 1\n",
    "                                ]\n",
    "        \n",
    "        if self._layers >= 3: \n",
    "            self.conv3, self.bn3, self._dimensions = self.conv_layer(self._kernel)\n",
    "            \n",
    "        \n",
    "        if self._layers >= 4: \n",
    "            self.conv4, self.bn4, self._dimensions = self.conv_layer(self._kernel)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = int(self._dimensions[2] * self._dimensions[3]), out_features = self._n_classes)\n",
    "        if self._print_dimensions == True:\n",
    "            print(\"Defined fc1\")\n",
    "            \n",
    "    def conv_layer(self, kernel_size):\n",
    "        conv = nn.Conv2d(in_channels = self._dimensions[1], out_channels = self._dimensions[1], kernel_size = kernel_size)\n",
    "        \n",
    "        dimensions = [self._dimensions[0],\n",
    "                            self._dimensions[1] ,\n",
    "                            (self._dimensions[2] - self._kernel + 2 * self._padding) * self._stride_inv + 1,\n",
    "                            (self._dimensions[3] - self._kernel + 2 * self._padding) * self._stride_inv + 1\n",
    "                            ]\n",
    "        \n",
    "        bn = nn.BatchNorm2d(self._dimensions[1])\n",
    "        \n",
    "        return conv, bn, dimensions\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def apply_conv_layer(self, input, conv, bn):\n",
    "        output = conv(input)\n",
    "        output = bn(output) \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        return output \n",
    "    \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        if self._print_dimensions == True:\n",
    "            print(f\"Input size: {input.size()}\")\n",
    "            \n",
    "        output = input\n",
    "        \n",
    "        if self._layers >= 1: \n",
    "            output = self.apply_conv_layer(output, self.conv1, self.bn1) \n",
    "        \n",
    "        if self._layers >= 2:\n",
    "            output = self.apply_conv_layer(output, self.conv2, self.bn2) \n",
    "        \n",
    "        \n",
    "        if self._pooling == True:\n",
    "            output = self.pool(output)    \n",
    "            if self._print_dimensions == True:\n",
    "                print(f\"Applied pool, size: {output.size()}\")\n",
    "        \n",
    "        \n",
    "        if self._layers >= 3:\n",
    "            output = self.apply_conv_layer(output, self.conv3, self.bn3) \n",
    "        \n",
    "        \n",
    "        if self._layers >= 4:\n",
    "            output = self.apply_conv_layer(output, self.conv4, self.bn4) \n",
    "        \n",
    "\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        if self._print_dimensions == True:\n",
    "            print(f\"Applied view, size: {output.size()}\")\n",
    "        \n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        if self._print_dimensions == True:\n",
    "            print(f\"Applied fc1, size: {output.size()}\") \n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "model = CNN(n_classes = 10, kernel_size = 4, input_size=[7000, 1, 28, 28], layers = 2, pooling = True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "# optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "trained_acc, trained_loss, model, batch_losses = train_model(epochs = 3,\n",
    "                                                             train_loader = data_train_loader,\n",
    "                                                             test_loader = data_test_loader,\n",
    "                                                             criterion = criterion,\n",
    "                                                             optimizer = optimizer,\n",
    "                                                             device = device,\n",
    "                                                             model = model)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(data_test_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Wrap-up!\n",
    "\n",
    "Evaluate the overall accuracy of the pipeline on the *test set* of *FashionMNIST*. *I.e.* compare the predicted labels from the three classifiers built in *Section 3* with the true labels.\n",
    "\n",
    "In order to assign a true label *name* (e.g. *trousers*, *sandal*, ...) to those determined just from *(kernel-)PCA* (that obviously carry no direct information about the subject of the picture), you can either:\n",
    "\n",
    "i. *Cheat* and use the most abundant labels for each group of *(kernel-)PCA-labelled* datapoints.\n",
    "\n",
    "ii. Sample a subset of datapoints from each *(kernel-)PCA-labelled* class, and assign one label by direct visual inspection. If you choose this route, it may also serve as a reminder of the fact that *expert labelling* is not always a trivial (and almost never a fast) task!\n",
    "\n",
    "Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_subset_scaled.shape)\n",
    "print(data_pca_sigmoid.shape)\n",
    "print(labels_SVM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class(class_number):\n",
    "    indeces = []\n",
    "    # Sampling only from class 1\n",
    "    for i in range(30):\n",
    "        while labels_SVM[i] != class_number:\n",
    "            i = np.random.choice(len(train_subset_scaled), size=1, replace=False)[0]\n",
    "        indeces.append(i)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 3))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indeces):\n",
    "        axs[i].imshow(\n",
    "            train_subset_scaled[idx].reshape(28, 28), cmap=\"gray\"\n",
    "        )  # Assuming images are 28x28 pixels\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"Showing class \" + str(i) + \":\")\n",
    "    show_class(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we choose the following mapping:\n",
    "| Class | Object |\n",
    "|-------|--------|\n",
    "| 0 | |\n",
    "| 1 | |\n",
    "| 2 | |\n",
    "| 3 | |\n",
    "| 4 | |\n",
    "| 5 | |\n",
    "| 6 | |\n",
    "| 7 | |\n",
    "| 8 | |\n",
    "| 9 | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: A *fully-supervised* approach\n",
    "\n",
    "Repeat the steps of *Section 3* using the true labels of the dataset. Comment on the results, and draw a comparison between such results and those obtained from the previous *hybrid* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset again\n",
    "\n",
    "subset_size = 64\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0, std=1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0, std=1)]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, shuffle=True, batch_size = subset_size)\n",
    "test_loader = DataLoader(dataset = test_dataset, shuffle=True, batch_size = subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing linear and rbf SVM and confronting it with the sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearSVM(nn.Module):\n",
    "#     def __init__(self, n_classes, image_size):\n",
    "#         super(LinearSVM, self).__init__()\n",
    "\n",
    "#         self._linear = nn.Linear(image_size, n_classes)\n",
    "            \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.flatten(start_dim = 1)\n",
    "#         x = self._linear(x)\n",
    "#         x = F.log_softmax(x, dim = 1)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model = LinearSVM(n_classes = 10, image_size = 28 * 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "\n",
    "# model = model.eval()\n",
    "\n",
    "# untrained_loss, untrained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "\n",
    "# model = model.train()\n",
    "\n",
    "# print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "# trained_acc, trained_loss, model, batch_losses = train_model(epochs = 3,\n",
    "#                                                              train_loader = train_loader,\n",
    "#                                                              test_loader = test_loader,\n",
    "#                                                              criterion = criterion,\n",
    "#                                                              optimizer = optimizer,\n",
    "#                                                              device = device,\n",
    "#                                                              model = model)\n",
    "# print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\").fit(train_subset_scaled, labels_subset)\n",
    "\n",
    "label_predict = classifier.predict(test_subset_scaled)\n",
    "accuracy_score(labels_test, label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RBF_SVM(nn.Module):\n",
    "#     def __init__(self, gamma_init, n_classes, image_size):\n",
    "#         super(RBF_SVM, self).__init__()\n",
    "#         self._gamma = nn.Parameter(th.FloatTensor(gamma_init), requires_grad = True)\n",
    "#         # self._bias = nn.Parameter(th.zeros(1, n_classes))\n",
    "#         self._support_vectors = nn.Parameter(th.randn(image_size, n_classes))\n",
    "    \n",
    "#     def rbf_kernel(self, x):\n",
    "#         diff = x[:, None, :] - self._support_vectors.t()[None, :, :]\n",
    "#         return th.exp(-self._gamma * (diff ** 2).sum( dim = 2))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.flatten(start_dim = 1)\n",
    "#         x = self.rbf_kernel(x)\n",
    "#         x = F.log_softmax(x, dim = 1)\n",
    "#         return x\n",
    "    \n",
    "# model = RBF_SVM(gamma_init = 1, n_classes = 10, image_size = 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "\n",
    "# model = model.eval()\n",
    "\n",
    "# untrained_loss, untrained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "\n",
    "# model = model.train()\n",
    "\n",
    "# print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "# trained_acc, trained_loss, model, batch_losses = train_model(epochs = 3,\n",
    "#                                                              train_loader = train_loader,\n",
    "#                                                              test_loader = test_loader,\n",
    "#                                                              criterion = criterion,\n",
    "#                                                              optimizer = optimizer,\n",
    "#                                                              device = device,\n",
    "#                                                              model = model)\n",
    "# print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"rbf\").fit(train_subset_scaled, labels_subset)\n",
    "\n",
    "label_predict = classifier.predict(test_subset_scaled)\n",
    "accuracy_score(labels_test, label_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNN(image_dim = 28 * 28, n_classes = 10)\n",
    "\n",
    "# Device selection\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")\n",
    "\n",
    "\n",
    "trained_acc, trained_loss, model, batch_losses = train_model(epochs = 3,\n",
    "                                                             train_loader = train_loader,\n",
    "                                                             test_loader = test_loader,\n",
    "                                                             criterion = criterion,\n",
    "                                                             optimizer = optimizer,\n",
    "                                                             device = device,\n",
    "                                                             model = model)\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_losses = []\n",
    "\n",
    "# for epoch in trange(2):\n",
    "#     model = model.train()\n",
    "\n",
    "#     # Actual (batch-wise) training step\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         logits = model(images)\n",
    "#         loss = criterion(logits, labels)\n",
    "\n",
    "#         batch_losses.append(loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get test stats after training\n",
    "# trained_loss, trained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "\n",
    "# print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "xrange = range(len(batch_losses))\n",
    "_ = plt.plot(xrange, batch_losses)\n",
    "_ = plt.scatter((xrange[0], xrange[-1]), (untrained_loss, trained_loss), color=\"red\")\n",
    "_ = plt.legend([\"Train Loss (batch)\", \"Test Loss (dataset)\"], loc=\"upper right\")\n",
    "_ = plt.xlabel(\"Number of batches seen\")\n",
    "_ = plt.ylabel(\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "model = CNN(n_classes = 10, kernel_size = 2, input_size = [64, 1, 28, 28], layers = 4, pooling = True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)\n",
    "\n",
    "# optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "untrained_loss, untrained_acc = get_test_stats(model, criterion, train_loader, device)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "print(f\"Untrained test loss: {untrained_loss:.4f}, accuracy: {untrained_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    model = model.train()\n",
    "\n",
    "    # Actual (batch-wise) training step\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        batch_losses.append(loss.item())  # Store the loss for plotting, per batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test stats after training\n",
    "trained_loss, trained_acc = get_test_stats(model, criterion, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy: {trained_acc} | Loss: {trained_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
